{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4,5,6장.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1-01loZQqxK5vYH9TKqn-eHexANyRIFMC",
      "authorship_tag": "ABX9TyOS00oSDJVEe61Ba+ih1JDw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/euna-jeong20/deep-learning-from-scratch/blob/main/4%2C5%2C6%EC%9E%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyWOS3supwDS"
      },
      "source": [
        "#4장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p85Wj9EwqgyG"
      },
      "source": [
        "## 4.2.1 오차제곱합\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4xNr2u4ptJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54393c45-e24c-42bf-99b8-59c2f1062a7a"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sum_squares_error(y, t):\n",
        "  return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "#정답은 2\n",
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "#예1 : 2일 확률이 가장 높다고 추정함\n",
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
        "\n",
        "print(sum_squares_error(np.array(y), np.array(t)))\n",
        "\n",
        "#예2 : 7일 확률이 가장 높다고 추정함\n",
        "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
        "print(sum_squares_error(np.array(y), np.array(t)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.09750000000000003\n",
            "0.5975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6YR1pmTrE8B"
      },
      "source": [
        "## 4.2.2 교차 엔트로피 오차"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D50wbFvyB_ro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaa7cc9a-321e-4a0b-8a0c-2b33b8fb99cc"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "  delta = 1*e - 7  #로그 0은 계산 될 수 없으니까 젤 작은값을 임의로 설정\n",
        "  return -np.sum(t * np.log(y + delta))\n",
        "\n",
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "\n",
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
        "print(sum_squares_error(np.array(y), np.array(t)))\n",
        "\n",
        "\n",
        "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
        "print(sum_squares_error(np.array(y), np.array(t)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.09750000000000003\n",
            "0.5975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlyV5sq7r1NF"
      },
      "source": [
        "## 4.2.3 미니배치 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtle-Z8nr7N_"
      },
      "source": [
        "* 훈련 데이터로부터 일부만 골라 학습을 수행하는 것을 미니배치 학습이라고 한다.\n",
        "* 예로 60,000장의 훈련 데이터 중에서 100장을 무작위로 뽑아서 그 100장을 가지고 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6ShT-CUDyn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2684f544-4ecb-4f7c-fc9c-34815aa877b5"
      },
      "source": [
        "import numpy as np\n",
        "import sys,os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master\")\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master')\n",
        "from dataset.mnist import load_mnist\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True) #flatten은 true로 자동 설정\n",
        "\n",
        "print(x_train.shape)  #(60000, 784(28X28))\n",
        "print(t_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDeRuy7WF_gm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31eb964-0a41-427c-8a42-e4c8907d0d16"
      },
      "source": [
        "# 훈련데이터에서 무작위로 10장만 빼내기\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 10\n",
        "batch_mask = np.random.choice(train_size, batch_size) # 0 부터 59999의 숫자중에서 10개를 뽑는다 -> 인덱스로 사용\n",
        "\n",
        "#x_batch = x_train[batch_mask]    xtrain에서 10개 임의로 추출해서 xbatch에 저장\n",
        "#t_batch = t_train[batch_mask]\n",
        "\n",
        "print(np.random.choice(60000,10))\n",
        "print(batch_mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[51007 20656 41051  5989  2973 56320 44786 19294 13071 57467]\n",
            "[26903 14521 39412 24000 57982 20579  4281 42596 48801 45504]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJPfiI2rs250"
      },
      "source": [
        "## 4.2.4 (배치용) 교차 엔트로피 오차 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlcNbNEnG9g4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c775a1b0-a9a7-46de-cdc7-b9192e9eda47"
      },
      "source": [
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:           # 데이터가 하나인 경우 (10, )\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshape(1, y.size)        #(1, 10)이 된다\n",
        "\n",
        "  batch_size = y.shape[0]\n",
        "  return - np.sum( t*np.log(y + 1*e - 7)) / batch_size  \n",
        "\n",
        "print(batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjqK06xSIMBA"
      },
      "source": [
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshpe(1, y.size)\n",
        "\n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum( t*np.log(y[np.arange(batch_size), t] + 1*e - 7)) / batch_size  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOi8g358JtUF"
      },
      "source": [
        "#예시 해서 봐봐"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJISyPpzukK9"
      },
      "source": [
        "## 4.3.1 미분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MIHt0tyvWc7"
      },
      "source": [
        "#나쁜 구현의 예\n",
        "def numerical_diff(f, x):\n",
        "  h = 10e-50\n",
        "  return ( f(x+h) - f(x) ) / h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWlbHx7pupna"
      },
      "source": [
        "* 나쁜점 1 : 반올림 오차 (엄청 작은 값(1e-50)을 float32형으로 나타내면 0으로 인식 그래서 1e-4사용)\n",
        "* 나쁜점 2 : (x + h) 와 x 사이의 기울기라서 약간의 오차가 생긴다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_1JgrZdId4e"
      },
      "source": [
        "def numerical_diff(f, x):\n",
        "  h = 1e-4\n",
        "  return (f(x+h) - f(x-h)) / (2*h)      #중앙차분"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjvilScFvn53"
      },
      "source": [
        "## 4.3.2 수치 미분의 예"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_umEzT-QLtyV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "b98712da-8d9e-4c0a-9952-879e69fc2999"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def function_1(x):\n",
        "  return 0.01*x**2 + 0.1*x\n",
        "\n",
        "x = np.arange(0.0, 20.0, 0.1)\n",
        "y = function_1(x)\n",
        "\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.show()\n",
        "\n",
        "print(numerical_diff(function_1, 5))\n",
        "print(numerical_diff(function_1, 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.1999999999990898\n",
            "0.2999999999986347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEENLdBewGPY"
      },
      "source": [
        "## 4..3 편미분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE_XA0Skw5Hk"
      },
      "source": [
        "def function_2(x):\n",
        "  return x[0]**2 + x[1]**2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjzYgfIZw9B5"
      },
      "source": [
        "* (3, 4)에서 각각 변수에 대한 편미분 구하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhy0RFQqMg7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88fa8745-658c-44c4-ed11-a131db353f71"
      },
      "source": [
        "def function_tmp1(x0):\n",
        "  return x0*x0 + 4.0**2.0\n",
        "\n",
        "print(numerical_diff(function_tmp1, 3.0))\n",
        "\n",
        "def function_tmp2(x1):\n",
        "  return 3.0**2.0 + x1*x1\n",
        "\n",
        "print(numerical_diff(function_tmp2, 4.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.00000000000378\n",
            "7.999999999999119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKtfT5etxV-E"
      },
      "source": [
        "## 4.4 기울기(그래디언트)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JieQc5ASN_Mq"
      },
      "source": [
        "def numerical_gradient(f, x):\n",
        "  h = 1e-4\n",
        "  grad = np.zeros_like(x)\n",
        "\n",
        "  for idx in range(x.size):\n",
        "    tmp_val = x[idx]            #기존의 x값\n",
        "    \n",
        "    x[idx] = tmp_val + h        #x+h\n",
        "    fxh1 = f(x)                 #f(x+h)\n",
        "\n",
        "    #f(x-h) 계산\n",
        "    x[idx] = tmp_val - h        #x-h\n",
        "    fxh2 = f(x)                 #f(x-h)\n",
        "\n",
        "    grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "    x[idx] = tmp_val\n",
        "\n",
        "  return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sg_XuWJyUEa"
      },
      "source": [
        "## 4.4.1 경사법(경사 하강법)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bmGqjOHyhXt"
      },
      "source": [
        "* 경사법은 최솟값을 찾을때에는 경사 하강법이라하고, 최댓값을 찾을때에는 경사 상승법이라고 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MbHfpnIXVGh"
      },
      "source": [
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
        "  x = init_x\n",
        "\n",
        "  for i in range(step_num):\n",
        "    grad = numerical_gradient(f, x)\n",
        "    x -= lr * grad\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VNXIXFKare-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3800276-46aa-41ac-f6b9-6ee58050d802"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def function_2(x):\n",
        "  return x[0]**2 + x[1]**2\n",
        "\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "gradient_descent(function_2, init_x = init_x, lr=0.1, step_num = 100)       #(0,0)에 가까운 값이 나오고 실제로 이 함수는 (0,0)에서 최솟값을 갖는다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-6.11110793e-10,  8.14814391e-10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrqhBoDIz0Zn",
        "outputId": "d2ea40de-77c7-4396-d33c-e49ce47ed2fe"
      },
      "source": [
        "#학습률이 너무 큰 예 : lr=10.0\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "print(gradient_descent(function_2, init_x = init_x, lr=10.0, step_num=100))\n",
        "\n",
        "#학습률이 너무 작은 예 : lr=1e-10\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "print(gradient_descent(function_2, init_x = init_x, lr=1e-10, step_num=100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2.58983747e+13 -1.29524862e+12]\n",
            "[-2.99999994  3.99999992]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_ImFwBE0ks0"
      },
      "source": [
        "## 4.4.2 신경망에서의 기울기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty_wOgy4eXBc"
      },
      "source": [
        "* 여기서의 기울기는 가중치 매개변수에 대한 손실 함수의 기울기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCzOjpBbbSFH"
      },
      "source": [
        "import numpy as np\n",
        "import sys,os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master\")\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master')\n",
        "\n",
        "from common.functions import softmax, cross_entropy_error\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "class simpleNet:\n",
        "  def __init__(self):\n",
        "    self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
        "\n",
        "  def predict(self, x):\n",
        "    return np.dot(x, self.W)    #예측에서 softmax를 사용하지 않아서 손실함수에서 사용\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    z = self.predict(x)\n",
        "    y = softmax(z)\n",
        "    loss = cross_entropy_error(y, t)\n",
        "\n",
        "    return loss "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fw_zysqfIBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1e10aa8-4eff-4726-fc53-8e73417ccf94"
      },
      "source": [
        "net = simpleNet()\n",
        "print(net.W)        #가중치 매개변수\n",
        "\n",
        "x = np.array([0.6, 0.9])\n",
        "p = net.predict(x)\n",
        "\n",
        "print(p)\n",
        "print(np.argmax(p))     #최댓값의 인덱스\n",
        "\n",
        "t = np.array([0, 0, 1]) #정답 레이블\n",
        "print(net.loss(x, t))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.09286272  0.62021819 -0.16067901]\n",
            " [ 1.03127474 -0.00835278 -0.07892151]]\n",
            "[ 0.87242964  0.36461342 -0.16743676]\n",
            "0\n",
            "1.7104147344938752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYJpTOGGnrYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7020e19d-3f54-45ef-9eda-3b2d360a3b3b"
      },
      "source": [
        "def f(W):       #여기에서 정의한 f(w) 함수의 인수 w는 더미로 만든것\n",
        "  return net.loss(x, t)\n",
        "\n",
        "dW = numerical_gradient(f, net.W)\n",
        "print(dW)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.3068565   0.18466881 -0.49152531]\n",
            " [ 0.46028475  0.27700321 -0.73728796]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZgfSmscgOvN"
      },
      "source": [
        "## 4.5.1 2층 신경망 클래스 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCcd7vOYn8Dz"
      },
      "source": [
        "import numpy as np\n",
        "import sys,os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master\")\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master')\n",
        "\n",
        "from common.functions import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "class TwoLayerNet:\n",
        "  \n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "    #가중치 초기화\n",
        "    self.params = {}    #딕셔너리를 하나 만듬\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "  def predict(self, x):     #추론\n",
        "    W1, W2 = self.params['W1'], self.params['W2']\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\n",
        "\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    y = softmax(a2)     #앞과는 달리 softmax함수를 추론에 사용, 그러므로 loss에서 사용안해도된다.\n",
        "\n",
        "    return y\n",
        "\n",
        "  # x : 입력 데이터, t : 정답 레이블\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "\n",
        "    return cross_entropy_error(y, t)\n",
        "\n",
        "  \n",
        "  def accuracy(self, x, t):         #학습할때 사용x\n",
        "    y = self.predict(x)\n",
        "    y = np.ardmax(y, axis=1)\n",
        "    t = np.argmax(t, axis=1)        #t가 원핫인코딩임을 알 수 있다.\n",
        "\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "    return accuracy\n",
        "  \n",
        "   # x : 입력 데이터, t : 정답 레이블\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqm9VJCjSzf_"
      },
      "source": [
        "## 4.5.2 미니배치 학습 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nvamMj4vaZF"
      },
      "source": [
        "import numpy as np\n",
        "import sys,os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master/ch04\")\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master/ch04')\n",
        "\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True, flatten=True)\n",
        "\n",
        "train_loss_list = []\n",
        "\n",
        "#하이퍼파라미터   수동으로 넣어야 하는 매개변수\n",
        "\n",
        "iters_num = 10000  #반복 횟수\n",
        "train_size = x_train.shape[0]       #60000개\n",
        "batch_size = 100  #미니배치크기\n",
        "learning_rate = 0.1\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  #미니배치 획득\n",
        "  batch_mask = np.random.choice(train_size, batch_size)     #0~59999 숫자중 100개 선택\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  #기울기 계산\n",
        "  grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "  #매개변수 갱신\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):      #각 가중치마다 갱신 해줘야 함\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  #학습경과 기록\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DOa9ex1zGW6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "98f8cde5-a820-4fb2-ac00-97001d833444"
      },
      "source": [
        "import numpy as np\n",
        "import sys,os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master/ch04\")\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master/ch04')\n",
        "\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "\n",
        "#하이퍼파라미터   수동으로 넣어야 하는 매개변수\n",
        "iters_num = 10000  #반복 횟수\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100  #미니배치크기\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "#1에폭당 반복 수\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  #미니배치 획득\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  #기울기 계산\n",
        "  grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "  #매개변수 갱신\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  #학습경과 기록\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "\n",
        "  #1에폭당 정확도 계산\n",
        "  if i % iter_per_epoch == 0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(\"train acc, test acc : str(train_acc) , str(test_acc)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train acc, test acc : str(train_acc) , str(test_acc)\n",
            "train acc, test acc : str(train_acc) , str(test_acc)\n",
            "train acc, test acc : str(train_acc) , str(test_acc)\n",
            "train acc, test acc : str(train_acc) , str(test_acc)\n",
            "train acc, test acc : str(train_acc) , str(test_acc)\n",
            "train acc, test acc : str(train_acc) , str(test_acc)\n",
            "train acc, test acc : str(train_acc) , str(test_acc)\n",
            "train acc, test acc : str(train_acc) , str(test_acc)\n",
            "train acc, test acc : str(train_acc) , str(test_acc)\n",
            "train acc, test acc : str(train_acc) , str(test_acc)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-65bd25534536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;31m#기울기 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;31m#매개변수 갱신\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master/ch04/two_layer_net.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mda1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mdz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mda1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdz1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cSKlBlMbKeP"
      },
      "source": [
        "#5장 오차역전파법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcETytkUY5yd"
      },
      "source": [
        "* 역전파는 기울기 갱신이 아니다. 기울기 구하는 방법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM7gv2tG4797"
      },
      "source": [
        "class MulLayer:\n",
        "  def __init__(self):\n",
        "    self.x = None\n",
        "    self.y = None\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    out = x * y\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * self.y      #x와 y를 바꾼다.\n",
        "    dy = dout * self.x\n",
        "\n",
        "    return dx, dy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTHdqZ49b8sA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1545f72-9b5a-41c2-b2c0-0e5c31bd8426"
      },
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "tax = 1.1\n",
        "\n",
        "#계층들\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "#순전파  순전파로 저장을 해야 역전파를 알 수 있다\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "print(price)  #220"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "220.00000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhBDdydRchPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86fa3b3b-8799-4cb6-83ea-1feedd138d2b"
      },
      "source": [
        "#역전파\n",
        "dprice = 1\n",
        "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(dapple_price)\n",
        "print(dapple, dapple_num, dtax)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1\n",
            "2.2 110.00000000000001 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVfaY0JteZkQ"
      },
      "source": [
        "class AddLayer:\n",
        "  def __init__(self):\n",
        "    pass    #초기화가 필요하지 않아서\n",
        "  \n",
        "  def forward(self, x, y):\n",
        "    out = x + y\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):   #사실 백워드는 필요없다\n",
        "    dx = dout * 1 \n",
        "    dy = dout * 1\n",
        "\n",
        "    return dx, dy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZFmX0Nheujg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0759ab0-5f75-428e-c083-2d0da2fdc227"
      },
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "orange = 150\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "#계층들\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_orange_layer = MulLayer()\n",
        "add_apple_orange_layer = AddLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "#순전파\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num) #(1)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num)  #(2)\n",
        "all_price = add_apple_orange_layer.forward(apple_price, orange_price) #(3)\n",
        "price = mul_tax_layer.forward(all_price, tax) #(4)\n",
        "\n",
        "#역전파\n",
        "dprice = 1\n",
        "dall_price, dtax = mul_tax_layer.backward(dprice) # (4)\n",
        "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) #(3)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price) #(2)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price) #(1)\n",
        "\n",
        "print(price)  #715\n",
        "print(dapple_num, dapple, dorange, dorange_num, dtax)   # 110 2.2 3.3 165 650"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "715.0000000000001\n",
            "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhpsV4eFujzW"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def ReLU(x):\n",
        "  return np.maximum([x,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-B92lILwx50"
      },
      "source": [
        "class Relu:\n",
        "  def __init__(self):\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self,x):\n",
        "    self.mask = (x <= 0)    #bool형이 나온다, 0보다 작은거에 true를 준다\n",
        "    out = x.copy()          # 카피시킨것은 원본을 바꿔도 카피했던 그때 그값 그대로 유지함.\n",
        "    out[self.mask] = 0\n",
        "    \n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout[self.mask] = 0     # 0이하의 값만 0으로 바꿔준다.\n",
        "    dx = dout\n",
        "\n",
        "    return dx\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRfJE5SuxxOo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a571cd4a-4ee5-4104-b2c1-6cac87e1f02a"
      },
      "source": [
        "x = np.array( [[1.0, -0.5], [-2.0, 3.0]] )\n",
        "print(x)\n",
        "mask = (x <= 0)\n",
        "print(mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.  -0.5]\n",
            " [-2.   3. ]]\n",
            "[[False  True]\n",
            " [ True False]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEMbE53SzWro"
      },
      "source": [
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.out = None\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = 1 / (1+np.exp(-x))\n",
        "    self.out = out      #역전파에 사용할 y 저장\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "    return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPON8VcQg8Ca"
      },
      "source": [
        "## 5.6.1 Affine 계층"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Cn0iJwy1pbD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60843c0-1e4b-4668-ba9d-58ec690bba5b"
      },
      "source": [
        "X = np.random.rand(2)\n",
        "W = np.random.rand(2,3)\n",
        "B = np.random.rand(3)\n",
        "\n",
        "print(X.shape)  # (1, 2)\n",
        "print(W.shape)  # (2, 3)\n",
        "print(B.shape)  # (1, 3)\n",
        "\n",
        "Y = np.dot(X, W) + B\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2,)\n",
            "(2, 3)\n",
            "(3,)\n",
            "[0.64478654 0.54825872 0.6376374 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BWItNr62yba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caab44ef-48bc-4c4e-d9d5-d76fed6beaaf"
      },
      "source": [
        "X_dot_W = np.array( [[0, 0, 0], [10, 10, 10]])\n",
        "B = np.array([1, 2, 3])\n",
        "print(X_dot_W)\n",
        "print(X_dot_W + B)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0]\n",
            " [10 10 10]]\n",
            "[[ 1  2  3]\n",
            " [11 12 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOsYQqQp4a_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9ed5f6-033f-4e5d-c003-16e3a7385e13"
      },
      "source": [
        "dY = np.array( [[1, 2, 3], [4, 5, 6]] )\n",
        "print(dY)\n",
        "\n",
        "dB = np.sum(dY, axis=0) #열끼리 더한다\n",
        "print(dB)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "[5 7 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9MvUzq341Nk"
      },
      "source": [
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.x = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "  \n",
        "  def forward(self, x):\n",
        "    self.x = x\n",
        "    out = np.dot(x, self.W) + self.b\n",
        "\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dx = np.dot(dout, self.W.T)\n",
        "    self.dW = np.dot(self.x.T, dout)\n",
        "    self.db = np.sum(dout, axis = 0)\n",
        "\n",
        "    return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSDlNbAA6JlC"
      },
      "source": [
        "class SoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.loss = None #손실\n",
        "    self.y = None     #softmax 출력\n",
        "    self.t = None\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    self.t = t\n",
        "    self.y = softmax(x)\n",
        "    self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "    return self.loss\n",
        "  \n",
        "  def backward(self, dout = 1):\n",
        "    batch_size = self.t.shape[0]  #열의 개수\n",
        "    dx = (self.y - self.t) / batch_size\n",
        "    \n",
        "    return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOS1x1C1-E8V"
      },
      "source": [
        "import numpy as np\n",
        "import sys,os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master/ch04\")\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master/ch04')\n",
        "\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "\n",
        "    #가중치 초기화\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    #계층생성\n",
        "    self.layers = OrderdDict()  #\n",
        "    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "    self.layers['Relu1'] = Relu()\n",
        "    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "    self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "  def predict(self, x):\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  # x : 입력 데이터, t : 정답 레이블\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return self.lastLayer,forward(y, t)\n",
        "\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis=1)\n",
        "\n",
        "    if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "  # x :입력 데이터, t : 정답 레이블\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_graident(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_graident(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_graident(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_graident(loss_W, self.params['b2'])\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    #순전파\n",
        "    self.oss(x, t)\n",
        "\n",
        "    #역전파\n",
        "    dout = 1\n",
        "    dout = self.lastLayer.backward(dout)\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    last.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "\n",
        "    # 결과 저장\n",
        "    grads = {}\n",
        "    grads['W1'] = self.layers['Affine1'].dW \n",
        "    grads['b1'] = self.layers['Affine1'].db \n",
        "    grads['W2'] = self.layers['Affine2'].dW \n",
        "    grads['b2'] = self.layers['Affine2'].db \n",
        "\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sii9cHTALw41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5355c0-b4ad-4bb7-d9e1-aa9f9ab7fe6b"
      },
      "source": [
        "import numpy as np\n",
        "import sys,os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master/ch05\")\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master/ch05')\n",
        "\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "#데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label= True)\n",
        "\n",
        "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
        "\n",
        "x_batch = x_train[:3]   #0,1,2행 떼온다     (3, 784)\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "#각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\n",
        "\n",
        "for key in grad_numerical.keys():   #grad_numerical가 딕셔너리다\n",
        "  diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]))\n",
        "  print(key + \":\" + str(diff))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1:1.8346581235635745e-10\n",
            "b1:7.676037822143718e-10\n",
            "W2:7.129719107536182e-08\n",
            "b2:1.418580851550444e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq4hP14vcmT7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "ff11510f-573b-44e3-fd01-41beb08e426c"
      },
      "source": [
        "import numpy as np\n",
        "import sys,os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master/ch04\")\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master/ch04')\n",
        "\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "\n",
        "#하이퍼파라미터   수동으로 넣어야 하는 매개변수\n",
        "iters_num = 10000  #반복 횟수\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100  #미니배치크기\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "#1에폭당 반복 수\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  #미니배치 획득\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  #오차역전파법으로 기울기를 구한다. 수치미분으로 하면 엄청 느리다.\n",
        "  grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "  #매개변수 갱신\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  #학습경과 기록\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "\n",
        "  #1에폭당 정확도 계산\n",
        "  if i % iter_per_epoch == 0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
        "\n",
        "\n",
        "\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "\n",
        "plt.plot(x, train_acc_list, label='train acc')\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
        "\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train acc, test acc | 0.0993, 0.1032\n",
            "train acc, test acc | 0.7921166666666667, 0.7979\n",
            "train acc, test acc | 0.87775, 0.8798\n",
            "train acc, test acc | 0.8978666666666667, 0.9012\n",
            "train acc, test acc | 0.9075333333333333, 0.9108\n",
            "train acc, test acc | 0.9137833333333333, 0.9168\n",
            "train acc, test acc | 0.9190666666666667, 0.9216\n",
            "train acc, test acc | 0.9236166666666666, 0.9244\n",
            "train acc, test acc | 0.9270166666666667, 0.9283\n",
            "train acc, test acc | 0.93025, 0.9303\n",
            "train acc, test acc | 0.933, 0.9331\n",
            "train acc, test acc | 0.9358, 0.9355\n",
            "train acc, test acc | 0.9383333333333334, 0.9378\n",
            "train acc, test acc | 0.9401833333333334, 0.9389\n",
            "train acc, test acc | 0.94375, 0.9424\n",
            "train acc, test acc | 0.9451833333333334, 0.9419\n",
            "train acc, test acc | 0.94635, 0.9448\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvjwrZGTo2u3"
      },
      "source": [
        "#6장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPVi1_sHgq6w"
      },
      "source": [
        "class SGD:\n",
        "  def __init__(self, lr=0.01):\n",
        "    self.lr = lr\n",
        "  \n",
        "  def update(self, params, grads):\n",
        "    for key in params.keys():\n",
        "      params[key] -= self.lr * grads[key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJMAzv-jjSxC"
      },
      "source": [
        "class Momentum:\n",
        "  def __init__(self, lr=0.01, momentum=0.9):\n",
        "    self.lr = lr\n",
        "    self.momentum = Momentum\n",
        "    self.v = None\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    if self.v is None:\n",
        "      self.v = {}\n",
        "\n",
        "      for key, val in params.items():\n",
        "        self.v[key] = np.zeros_like(val)\n",
        "\n",
        "      for key in params.key():\n",
        "        self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
        "        params[key] += self.v[key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r9tAIkqlDLh"
      },
      "source": [
        "class AdaGrad:    #학습을 적게 할때 사용하면 좋다, 넘 많이 하다가 갱신량이 0이 될 수 있다.\n",
        "  def __init__(self, lr=0.01):\n",
        "    self.lr = lr\n",
        "    self.h = None\n",
        "  \n",
        "  def update(self, params, grads ):\n",
        "    if self.h is None:\n",
        "      self.h = {}\n",
        "\n",
        "      for key, val in params.items():\n",
        "        self.h[key] = np.zeros_like(val)\n",
        "\n",
        "      for key in params.key():\n",
        "        self.h[key] += grads[key] * grads[key]      #elementwise 곱\n",
        "        params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hXCCDZ3Sol-"
      },
      "source": [
        "#RMSProp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyZgA4EKTzU5"
      },
      "source": [
        "#Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Cc2VF6ZV50H"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNtfWbl_m78v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "e511896d-09e8-4952-a567-a1581adad8fb"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "x = np.random.randn(1000, 100)  #100개의 데이터\n",
        "node_num = 100                  #각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5           #은닉층이 5개\n",
        "activations = {}                #이곳에 홣성화 결과(활성화값)를 저장\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "  if i != 0:\n",
        "    x = activations[i-1]\n",
        "\n",
        "  w = np.random.randn(node_num, node_num) * 1   #표준편차가 1이라서 1을 곱한다 (100,100)\n",
        "  a = np.dot(x, w)          #(1000,100)\n",
        "  z = sigmoid(a)\n",
        "  activations[i] = z\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "for i, a in activations.items():\n",
        "   plt.subplot(1, len(activations), i+1)\n",
        "   plt.title(str(i+1) + \"-layer\")\n",
        "   plt.hist(a.flatten(), 30, range=(0,1))\n",
        "   plt.tight_layout(pad=0.01)\n",
        "   plt.ylim(0,40000)\n",
        "   plt.xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x216 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD-4mfiPxE3N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "8f764567-48d0-44b2-d9c7-96f136c4bf44"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "x = np.random.randn(1000, 100)  #100개의 데이터\n",
        "node_num = 100                  #각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5           #은닉층이 5개\n",
        "activations = {}                #이곳에 홣성화 결과(활성화값)를 저장\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "  if i != 0:\n",
        "    x = activations[i-1]\n",
        "\n",
        "  w = np.random.randn(node_num, node_num) / np.sqrt(node_num)   #Xavier 초기값을 사용/ sigmoid는 표준편차의 영향을 받는다\n",
        "  a = np.dot(x, w)\n",
        "  z = sigmoid(a)\n",
        "  activations[i] = z\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "for i, a in activations.items():\n",
        "   plt.subplot(1, len(activations), i+1)\n",
        "   plt.title(str(i+1) + \"-layer\")\n",
        "   plt.hist(a.flatten(), 30, range=(0,1))\n",
        "   plt.tight_layout(pad=0.01)\n",
        "   plt.ylim(0,40000)\n",
        "   plt.xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x216 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJS_kkqb1zcN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "7dd580b2-bf11-4245-b61f-f01717d39d52"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def Relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "x = np.random.randn(1000, 100)  #100개의 데이터\n",
        "node_num = 100                  #각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5           #은닉층이 5개\n",
        "activations = {}                #이곳에 홣성화 결과(활성화값)를 저장\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "  if i != 0:\n",
        "    x = activations[i-1]\n",
        "\n",
        "  w = np.random.randn(node_num, node_num) / np.sqrt(node_num / 2)   #He 초기값을 사용/ Relu는 표준편차의 영향을 받는다\n",
        "  a = np.dot(x, w)\n",
        "  z = Relu(a)\n",
        "  activations[i] = z\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "for i, a in activations.items():\n",
        "   plt.subplot(1, len(activations), i+1)\n",
        "   plt.title(str(i+1) + \"-layer\")\n",
        "   plt.hist(a.flatten(), 30, range=(0,1))\n",
        "   plt.tight_layout(pad=0.01)\n",
        "   plt.ylim(0,40000)\n",
        "   plt.xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x216 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y7sQW9e4atB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a4671a6-169e-4d31-c2e2-5881738582f4"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master\")\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.optimizer import SGD\n",
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True)\n",
        "\n",
        "x_train = x_train[:300]     #(300,784)\n",
        "t_train = t_train[:300]\n",
        "\n",
        "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10)    #은닉층은 6개 전체 층은 7개\n",
        "optimizer = SGD(lr=0.01)\n",
        "max_epochs = 201\n",
        "train_size = x_train.shape[0]   #300   /300으로 잘랐으니까\n",
        "batch_size = 100\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)    #3\n",
        "epoch_cnt = 0\n",
        "\n",
        "for i in range(1000000000):\n",
        "  batch_mask = np.random.choice(train_size, batch_size)   #t개 중에서 b개만큼 선택 미니배치\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  grads = network.gradient(x_batch, t_batch)    #오차역전파법\n",
        "  optimizer.update(network.params, grads)\n",
        "\n",
        "  if i % iter_per_epoch == 0:   #3의 배수\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
        "    epoch_cnt += 1\n",
        "    \n",
        "    if epoch_cnt >= max_epochs:\n",
        "      break\n",
        "\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0, train acc:0.09333333333333334, test acc:0.0788\n",
            "epoch:1, train acc:0.10333333333333333, test acc:0.0769\n",
            "epoch:2, train acc:0.12, test acc:0.0806\n",
            "epoch:3, train acc:0.13, test acc:0.0846\n",
            "epoch:4, train acc:0.15333333333333332, test acc:0.0887\n",
            "epoch:5, train acc:0.17333333333333334, test acc:0.1043\n",
            "epoch:6, train acc:0.20333333333333334, test acc:0.1177\n",
            "epoch:7, train acc:0.22333333333333333, test acc:0.1308\n",
            "epoch:8, train acc:0.25666666666666665, test acc:0.1472\n",
            "epoch:9, train acc:0.29, test acc:0.1629\n",
            "epoch:10, train acc:0.33, test acc:0.1773\n",
            "epoch:11, train acc:0.3566666666666667, test acc:0.1938\n",
            "epoch:12, train acc:0.36333333333333334, test acc:0.2078\n",
            "epoch:13, train acc:0.37, test acc:0.2194\n",
            "epoch:14, train acc:0.37666666666666665, test acc:0.235\n",
            "epoch:15, train acc:0.36333333333333334, test acc:0.2382\n",
            "epoch:16, train acc:0.4, test acc:0.2574\n",
            "epoch:17, train acc:0.44, test acc:0.2749\n",
            "epoch:18, train acc:0.4633333333333333, test acc:0.2911\n",
            "epoch:19, train acc:0.49666666666666665, test acc:0.3179\n",
            "epoch:20, train acc:0.5266666666666666, test acc:0.34\n",
            "epoch:21, train acc:0.5333333333333333, test acc:0.3462\n",
            "epoch:22, train acc:0.5533333333333333, test acc:0.3562\n",
            "epoch:23, train acc:0.55, test acc:0.3634\n",
            "epoch:24, train acc:0.5433333333333333, test acc:0.3707\n",
            "epoch:25, train acc:0.5766666666666667, test acc:0.3896\n",
            "epoch:26, train acc:0.5733333333333334, test acc:0.4017\n",
            "epoch:27, train acc:0.58, test acc:0.4094\n",
            "epoch:28, train acc:0.61, test acc:0.4243\n",
            "epoch:29, train acc:0.61, test acc:0.4398\n",
            "epoch:30, train acc:0.6133333333333333, test acc:0.442\n",
            "epoch:31, train acc:0.6266666666666667, test acc:0.4574\n",
            "epoch:32, train acc:0.6333333333333333, test acc:0.4672\n",
            "epoch:33, train acc:0.6633333333333333, test acc:0.4854\n",
            "epoch:34, train acc:0.66, test acc:0.4869\n",
            "epoch:35, train acc:0.6733333333333333, test acc:0.5005\n",
            "epoch:36, train acc:0.6966666666666667, test acc:0.5205\n",
            "epoch:37, train acc:0.7133333333333334, test acc:0.5362\n",
            "epoch:38, train acc:0.7266666666666667, test acc:0.5447\n",
            "epoch:39, train acc:0.7433333333333333, test acc:0.5482\n",
            "epoch:40, train acc:0.76, test acc:0.5529\n",
            "epoch:41, train acc:0.77, test acc:0.5584\n",
            "epoch:42, train acc:0.7933333333333333, test acc:0.5687\n",
            "epoch:43, train acc:0.8233333333333334, test acc:0.5688\n",
            "epoch:44, train acc:0.8133333333333334, test acc:0.5804\n",
            "epoch:45, train acc:0.8333333333333334, test acc:0.5786\n",
            "epoch:46, train acc:0.8366666666666667, test acc:0.5908\n",
            "epoch:47, train acc:0.83, test acc:0.587\n",
            "epoch:48, train acc:0.8366666666666667, test acc:0.5937\n",
            "epoch:49, train acc:0.8366666666666667, test acc:0.5966\n",
            "epoch:50, train acc:0.85, test acc:0.6058\n",
            "epoch:51, train acc:0.8633333333333333, test acc:0.6187\n",
            "epoch:52, train acc:0.8566666666666667, test acc:0.6178\n",
            "epoch:53, train acc:0.87, test acc:0.6226\n",
            "epoch:54, train acc:0.8866666666666667, test acc:0.6315\n",
            "epoch:55, train acc:0.9066666666666666, test acc:0.6269\n",
            "epoch:56, train acc:0.9033333333333333, test acc:0.6335\n",
            "epoch:57, train acc:0.9, test acc:0.6384\n",
            "epoch:58, train acc:0.9133333333333333, test acc:0.6428\n",
            "epoch:59, train acc:0.9033333333333333, test acc:0.6433\n",
            "epoch:60, train acc:0.91, test acc:0.6421\n",
            "epoch:61, train acc:0.92, test acc:0.6491\n",
            "epoch:62, train acc:0.9233333333333333, test acc:0.6444\n",
            "epoch:63, train acc:0.9266666666666666, test acc:0.6535\n",
            "epoch:64, train acc:0.9366666666666666, test acc:0.6499\n",
            "epoch:65, train acc:0.9366666666666666, test acc:0.6564\n",
            "epoch:66, train acc:0.9366666666666666, test acc:0.6596\n",
            "epoch:67, train acc:0.9433333333333334, test acc:0.6576\n",
            "epoch:68, train acc:0.9366666666666666, test acc:0.6616\n",
            "epoch:69, train acc:0.94, test acc:0.6592\n",
            "epoch:70, train acc:0.95, test acc:0.6693\n",
            "epoch:71, train acc:0.9566666666666667, test acc:0.6639\n",
            "epoch:72, train acc:0.9533333333333334, test acc:0.6676\n",
            "epoch:73, train acc:0.9533333333333334, test acc:0.6698\n",
            "epoch:74, train acc:0.9566666666666667, test acc:0.6692\n",
            "epoch:75, train acc:0.95, test acc:0.6686\n",
            "epoch:76, train acc:0.9566666666666667, test acc:0.6734\n",
            "epoch:77, train acc:0.9566666666666667, test acc:0.6722\n",
            "epoch:78, train acc:0.9566666666666667, test acc:0.6756\n",
            "epoch:79, train acc:0.9633333333333334, test acc:0.6744\n",
            "epoch:80, train acc:0.9566666666666667, test acc:0.6847\n",
            "epoch:81, train acc:0.9633333333333334, test acc:0.6819\n",
            "epoch:82, train acc:0.9666666666666667, test acc:0.6831\n",
            "epoch:83, train acc:0.97, test acc:0.6856\n",
            "epoch:84, train acc:0.97, test acc:0.6887\n",
            "epoch:85, train acc:0.97, test acc:0.6884\n",
            "epoch:86, train acc:0.97, test acc:0.6907\n",
            "epoch:87, train acc:0.9766666666666667, test acc:0.6906\n",
            "epoch:88, train acc:0.9766666666666667, test acc:0.6903\n",
            "epoch:89, train acc:0.9766666666666667, test acc:0.6944\n",
            "epoch:90, train acc:0.9733333333333334, test acc:0.6868\n",
            "epoch:91, train acc:0.9733333333333334, test acc:0.6863\n",
            "epoch:92, train acc:0.9733333333333334, test acc:0.6899\n",
            "epoch:93, train acc:0.9733333333333334, test acc:0.6902\n",
            "epoch:94, train acc:0.98, test acc:0.6925\n",
            "epoch:95, train acc:0.9766666666666667, test acc:0.6982\n",
            "epoch:96, train acc:0.9766666666666667, test acc:0.6951\n",
            "epoch:97, train acc:0.9733333333333334, test acc:0.6989\n",
            "epoch:98, train acc:0.98, test acc:0.6989\n",
            "epoch:99, train acc:0.98, test acc:0.6995\n",
            "epoch:100, train acc:0.98, test acc:0.7006\n",
            "epoch:101, train acc:0.98, test acc:0.7004\n",
            "epoch:102, train acc:0.98, test acc:0.7045\n",
            "epoch:103, train acc:0.9833333333333333, test acc:0.7017\n",
            "epoch:104, train acc:0.98, test acc:0.7022\n",
            "epoch:105, train acc:0.98, test acc:0.6989\n",
            "epoch:106, train acc:0.98, test acc:0.7017\n",
            "epoch:107, train acc:0.9833333333333333, test acc:0.7074\n",
            "epoch:108, train acc:0.9833333333333333, test acc:0.7047\n",
            "epoch:109, train acc:0.9833333333333333, test acc:0.7045\n",
            "epoch:110, train acc:0.9866666666666667, test acc:0.7034\n",
            "epoch:111, train acc:0.9866666666666667, test acc:0.7042\n",
            "epoch:112, train acc:0.99, test acc:0.7058\n",
            "epoch:113, train acc:0.9866666666666667, test acc:0.7061\n",
            "epoch:114, train acc:0.9866666666666667, test acc:0.703\n",
            "epoch:115, train acc:0.9866666666666667, test acc:0.7067\n",
            "epoch:116, train acc:0.9833333333333333, test acc:0.7071\n",
            "epoch:117, train acc:0.99, test acc:0.7099\n",
            "epoch:118, train acc:0.99, test acc:0.7089\n",
            "epoch:119, train acc:0.99, test acc:0.7098\n",
            "epoch:120, train acc:0.99, test acc:0.7099\n",
            "epoch:121, train acc:0.9933333333333333, test acc:0.71\n",
            "epoch:122, train acc:0.9933333333333333, test acc:0.7115\n",
            "epoch:123, train acc:0.9933333333333333, test acc:0.7121\n",
            "epoch:124, train acc:0.99, test acc:0.7133\n",
            "epoch:125, train acc:0.9933333333333333, test acc:0.7117\n",
            "epoch:126, train acc:0.9933333333333333, test acc:0.7132\n",
            "epoch:127, train acc:0.9933333333333333, test acc:0.7116\n",
            "epoch:128, train acc:0.9933333333333333, test acc:0.7151\n",
            "epoch:129, train acc:0.9933333333333333, test acc:0.7128\n",
            "epoch:130, train acc:0.99, test acc:0.7136\n",
            "epoch:131, train acc:0.9933333333333333, test acc:0.7135\n",
            "epoch:132, train acc:0.9933333333333333, test acc:0.7102\n",
            "epoch:133, train acc:0.9933333333333333, test acc:0.7121\n",
            "epoch:134, train acc:0.9933333333333333, test acc:0.7143\n",
            "epoch:135, train acc:0.9966666666666667, test acc:0.7136\n",
            "epoch:136, train acc:0.9933333333333333, test acc:0.7139\n",
            "epoch:137, train acc:0.9933333333333333, test acc:0.7144\n",
            "epoch:138, train acc:1.0, test acc:0.7151\n",
            "epoch:139, train acc:0.9966666666666667, test acc:0.7158\n",
            "epoch:140, train acc:0.9966666666666667, test acc:0.7159\n",
            "epoch:141, train acc:0.9933333333333333, test acc:0.7156\n",
            "epoch:142, train acc:0.9933333333333333, test acc:0.7167\n",
            "epoch:143, train acc:0.9966666666666667, test acc:0.7174\n",
            "epoch:144, train acc:1.0, test acc:0.7166\n",
            "epoch:145, train acc:1.0, test acc:0.7172\n",
            "epoch:146, train acc:1.0, test acc:0.7194\n",
            "epoch:147, train acc:1.0, test acc:0.7192\n",
            "epoch:148, train acc:1.0, test acc:0.7186\n",
            "epoch:149, train acc:1.0, test acc:0.7186\n",
            "epoch:150, train acc:1.0, test acc:0.7203\n",
            "epoch:151, train acc:1.0, test acc:0.7178\n",
            "epoch:152, train acc:1.0, test acc:0.7197\n",
            "epoch:153, train acc:1.0, test acc:0.7184\n",
            "epoch:154, train acc:1.0, test acc:0.721\n",
            "epoch:155, train acc:1.0, test acc:0.7184\n",
            "epoch:156, train acc:1.0, test acc:0.7195\n",
            "epoch:157, train acc:1.0, test acc:0.7188\n",
            "epoch:158, train acc:1.0, test acc:0.7231\n",
            "epoch:159, train acc:1.0, test acc:0.7215\n",
            "epoch:160, train acc:1.0, test acc:0.7196\n",
            "epoch:161, train acc:1.0, test acc:0.7222\n",
            "epoch:162, train acc:1.0, test acc:0.722\n",
            "epoch:163, train acc:1.0, test acc:0.7204\n",
            "epoch:164, train acc:1.0, test acc:0.7211\n",
            "epoch:165, train acc:1.0, test acc:0.7211\n",
            "epoch:166, train acc:1.0, test acc:0.7194\n",
            "epoch:167, train acc:1.0, test acc:0.7213\n",
            "epoch:168, train acc:1.0, test acc:0.7215\n",
            "epoch:169, train acc:1.0, test acc:0.7221\n",
            "epoch:170, train acc:1.0, test acc:0.7229\n",
            "epoch:171, train acc:1.0, test acc:0.7224\n",
            "epoch:172, train acc:1.0, test acc:0.7213\n",
            "epoch:173, train acc:1.0, test acc:0.7218\n",
            "epoch:174, train acc:1.0, test acc:0.7242\n",
            "epoch:175, train acc:1.0, test acc:0.7248\n",
            "epoch:176, train acc:1.0, test acc:0.724\n",
            "epoch:177, train acc:1.0, test acc:0.7228\n",
            "epoch:178, train acc:1.0, test acc:0.7241\n",
            "epoch:179, train acc:1.0, test acc:0.7257\n",
            "epoch:180, train acc:1.0, test acc:0.7245\n",
            "epoch:181, train acc:1.0, test acc:0.7255\n",
            "epoch:182, train acc:1.0, test acc:0.724\n",
            "epoch:183, train acc:1.0, test acc:0.7246\n",
            "epoch:184, train acc:1.0, test acc:0.7241\n",
            "epoch:185, train acc:1.0, test acc:0.7257\n",
            "epoch:186, train acc:1.0, test acc:0.7254\n",
            "epoch:187, train acc:1.0, test acc:0.7244\n",
            "epoch:188, train acc:1.0, test acc:0.7269\n",
            "epoch:189, train acc:1.0, test acc:0.7269\n",
            "epoch:190, train acc:1.0, test acc:0.7284\n",
            "epoch:191, train acc:1.0, test acc:0.7268\n",
            "epoch:192, train acc:1.0, test acc:0.7297\n",
            "epoch:193, train acc:1.0, test acc:0.7296\n",
            "epoch:194, train acc:1.0, test acc:0.7292\n",
            "epoch:195, train acc:1.0, test acc:0.73\n",
            "epoch:196, train acc:1.0, test acc:0.7305\n",
            "epoch:197, train acc:1.0, test acc:0.7313\n",
            "epoch:198, train acc:1.0, test acc:0.7294\n",
            "epoch:199, train acc:1.0, test acc:0.7299\n",
            "epoch:200, train acc:1.0, test acc:0.729\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwmGbTl3A3a9"
      },
      "source": [
        "class Dropout :\n",
        "  def __init__(self, dropout_ratio = 0.5):\n",
        "    self.dropout_ration = dropout_ratio\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, x, train_flg=True):\n",
        "    if train_flg: #훈련 때\n",
        "      self.mask + np.random.rand(*x.shape) > self.dropout_ratio     #*x.shape 받을 매개변수가 튜플이다\n",
        "      return x * self.mask\n",
        "    else:          #훈련 x\n",
        "      return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "      return dout * self.mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhmO5rmMtL8k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5dbdec0a-aab4-4178-9ef1-af6a0f9a239b"
      },
      "source": [
        "# coding: utf-8\n",
        "# p220 드롭아웃의 효과를 mnist 데이터셋으로 확인\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
        "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
        "dropout_ratio = 0.2\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=301, mini_batch_size=100,\n",
        "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
        "trainer.train()\n",
        "\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss:2.3512894711775902\n",
            "=== epoch:1, train acc:0.09666666666666666, test acc:0.1104 ===\n",
            "train loss:2.325334245486249\n",
            "train loss:2.31513696851772\n",
            "train loss:2.3181513708191304\n",
            "=== epoch:2, train acc:0.09666666666666666, test acc:0.1094 ===\n",
            "train loss:2.316872806114976\n",
            "train loss:2.3364952575115066\n",
            "train loss:2.321582700742381\n",
            "=== epoch:3, train acc:0.09, test acc:0.1112 ===\n",
            "train loss:2.3215053600555557\n",
            "train loss:2.3115433511749424\n",
            "train loss:2.3022087359967243\n",
            "=== epoch:4, train acc:0.09, test acc:0.1112 ===\n",
            "train loss:2.319077538552417\n",
            "train loss:2.303030168507182\n",
            "train loss:2.3040407244723924\n",
            "=== epoch:5, train acc:0.09333333333333334, test acc:0.1119 ===\n",
            "train loss:2.3295714534918317\n",
            "train loss:2.281079521798453\n",
            "train loss:2.31715499922353\n",
            "=== epoch:6, train acc:0.09666666666666666, test acc:0.1106 ===\n",
            "train loss:2.308288248741825\n",
            "train loss:2.3290978626485317\n",
            "train loss:2.2945588553611262\n",
            "=== epoch:7, train acc:0.11, test acc:0.1114 ===\n",
            "train loss:2.304886671970794\n",
            "train loss:2.3136944842743836\n",
            "train loss:2.3215508295955662\n",
            "=== epoch:8, train acc:0.10333333333333333, test acc:0.1148 ===\n",
            "train loss:2.3157352509413136\n",
            "train loss:2.3039354744402627\n",
            "train loss:2.304322113466145\n",
            "=== epoch:9, train acc:0.11, test acc:0.1188 ===\n",
            "train loss:2.29846333176516\n",
            "train loss:2.296798534937936\n",
            "train loss:2.3147422102265263\n",
            "=== epoch:10, train acc:0.11333333333333333, test acc:0.1266 ===\n",
            "train loss:2.3103646293598534\n",
            "train loss:2.2948842013439332\n",
            "train loss:2.3087581198273166\n",
            "=== epoch:11, train acc:0.12333333333333334, test acc:0.1318 ===\n",
            "train loss:2.298762505885173\n",
            "train loss:2.297872391190236\n",
            "train loss:2.2881143320830613\n",
            "=== epoch:12, train acc:0.14, test acc:0.1385 ===\n",
            "train loss:2.2853270815834374\n",
            "train loss:2.3003258195023353\n",
            "train loss:2.294315350032194\n",
            "=== epoch:13, train acc:0.14333333333333334, test acc:0.1511 ===\n",
            "train loss:2.2796667569621922\n",
            "train loss:2.2799830214255823\n",
            "train loss:2.297488888338832\n",
            "=== epoch:14, train acc:0.14666666666666667, test acc:0.1584 ===\n",
            "train loss:2.292265570164533\n",
            "train loss:2.2878499230343503\n",
            "train loss:2.2894583387440903\n",
            "=== epoch:15, train acc:0.14666666666666667, test acc:0.1659 ===\n",
            "train loss:2.2934253269169154\n",
            "train loss:2.3009691340623912\n",
            "train loss:2.286706405975469\n",
            "=== epoch:16, train acc:0.15333333333333332, test acc:0.1656 ===\n",
            "train loss:2.275739303238725\n",
            "train loss:2.2816044540954596\n",
            "train loss:2.285456516761157\n",
            "=== epoch:17, train acc:0.16333333333333333, test acc:0.1697 ===\n",
            "train loss:2.2792076350139148\n",
            "train loss:2.2883437011847403\n",
            "train loss:2.286841897649285\n",
            "=== epoch:18, train acc:0.17333333333333334, test acc:0.1762 ===\n",
            "train loss:2.2881337516307285\n",
            "train loss:2.2861808321517643\n",
            "train loss:2.2755985626207678\n",
            "=== epoch:19, train acc:0.18666666666666668, test acc:0.1835 ===\n",
            "train loss:2.2977452724618064\n",
            "train loss:2.2749474551303224\n",
            "train loss:2.2900100591694934\n",
            "=== epoch:20, train acc:0.19, test acc:0.1864 ===\n",
            "train loss:2.283863436516828\n",
            "train loss:2.2601530421446485\n",
            "train loss:2.271337992676821\n",
            "=== epoch:21, train acc:0.19666666666666666, test acc:0.1865 ===\n",
            "train loss:2.2618305420394744\n",
            "train loss:2.266656385151798\n",
            "train loss:2.2707072152590824\n",
            "=== epoch:22, train acc:0.2, test acc:0.1901 ===\n",
            "train loss:2.2593710843559216\n",
            "train loss:2.267815089928605\n",
            "train loss:2.2837508409219556\n",
            "=== epoch:23, train acc:0.20333333333333334, test acc:0.1941 ===\n",
            "train loss:2.255419623604937\n",
            "train loss:2.2838615564380813\n",
            "train loss:2.2758427219152977\n",
            "=== epoch:24, train acc:0.20333333333333334, test acc:0.1918 ===\n",
            "train loss:2.2626811130583024\n",
            "train loss:2.2863684158115563\n",
            "train loss:2.2670678113654237\n",
            "=== epoch:25, train acc:0.20666666666666667, test acc:0.197 ===\n",
            "train loss:2.271071403037882\n",
            "train loss:2.272926981305529\n",
            "train loss:2.264842168257533\n",
            "=== epoch:26, train acc:0.21, test acc:0.2016 ===\n",
            "train loss:2.2790804548061927\n",
            "train loss:2.271630321801853\n",
            "train loss:2.2505125669614388\n",
            "=== epoch:27, train acc:0.22, test acc:0.2043 ===\n",
            "train loss:2.266497183546954\n",
            "train loss:2.2568809216199166\n",
            "train loss:2.244795416942837\n",
            "=== epoch:28, train acc:0.20666666666666667, test acc:0.2013 ===\n",
            "train loss:2.2700622985728143\n",
            "train loss:2.268319397429069\n",
            "train loss:2.259072854042467\n",
            "=== epoch:29, train acc:0.20333333333333334, test acc:0.1982 ===\n",
            "train loss:2.2428939579071407\n",
            "train loss:2.2614294804493587\n",
            "train loss:2.243963056511\n",
            "=== epoch:30, train acc:0.19666666666666666, test acc:0.1927 ===\n",
            "train loss:2.279695364502513\n",
            "train loss:2.2592750679294213\n",
            "train loss:2.2551526608653614\n",
            "=== epoch:31, train acc:0.2, test acc:0.1909 ===\n",
            "train loss:2.253247267759885\n",
            "train loss:2.2426926310368103\n",
            "train loss:2.240312553288255\n",
            "=== epoch:32, train acc:0.2, test acc:0.1866 ===\n",
            "train loss:2.259849822526116\n",
            "train loss:2.2557709028384103\n",
            "train loss:2.2744435053353262\n",
            "=== epoch:33, train acc:0.20666666666666667, test acc:0.188 ===\n",
            "train loss:2.244349656894006\n",
            "train loss:2.24160103027183\n",
            "train loss:2.2632275695476847\n",
            "=== epoch:34, train acc:0.2, test acc:0.1876 ===\n",
            "train loss:2.2504606013339763\n",
            "train loss:2.2376614309419516\n",
            "train loss:2.2661772877768263\n",
            "=== epoch:35, train acc:0.20333333333333334, test acc:0.1938 ===\n",
            "train loss:2.2453593381882286\n",
            "train loss:2.2407500212225258\n",
            "train loss:2.251239272430887\n",
            "=== epoch:36, train acc:0.20666666666666667, test acc:0.1977 ===\n",
            "train loss:2.2530422448032854\n",
            "train loss:2.242717442631678\n",
            "train loss:2.2398902873484956\n",
            "=== epoch:37, train acc:0.20666666666666667, test acc:0.1947 ===\n",
            "train loss:2.242786341847037\n",
            "train loss:2.255523393288557\n",
            "train loss:2.2502420715017326\n",
            "=== epoch:38, train acc:0.21, test acc:0.1988 ===\n",
            "train loss:2.230224643851272\n",
            "train loss:2.2302735003187606\n",
            "train loss:2.2549970407943976\n",
            "=== epoch:39, train acc:0.21666666666666667, test acc:0.1972 ===\n",
            "train loss:2.230977018904303\n",
            "train loss:2.2424526598781798\n",
            "train loss:2.2408610038737073\n",
            "=== epoch:40, train acc:0.21, test acc:0.1986 ===\n",
            "train loss:2.229695412606862\n",
            "train loss:2.2332716458853104\n",
            "train loss:2.2332975329732396\n",
            "=== epoch:41, train acc:0.21, test acc:0.1984 ===\n",
            "train loss:2.250402065102201\n",
            "train loss:2.219792831207383\n",
            "train loss:2.2328122646081225\n",
            "=== epoch:42, train acc:0.21333333333333335, test acc:0.1962 ===\n",
            "train loss:2.2317424146290428\n",
            "train loss:2.2311353294901624\n",
            "train loss:2.2340121846112346\n",
            "=== epoch:43, train acc:0.21, test acc:0.1977 ===\n",
            "train loss:2.2223034488781956\n",
            "train loss:2.2354797165081677\n",
            "train loss:2.213096495992847\n",
            "=== epoch:44, train acc:0.22, test acc:0.2003 ===\n",
            "train loss:2.250827430601074\n",
            "train loss:2.22615483779709\n",
            "train loss:2.2508397787873946\n",
            "=== epoch:45, train acc:0.22, test acc:0.2044 ===\n",
            "train loss:2.2092758416069067\n",
            "train loss:2.2266414592793313\n",
            "train loss:2.251889164793295\n",
            "=== epoch:46, train acc:0.23, test acc:0.2062 ===\n",
            "train loss:2.2131347096836524\n",
            "train loss:2.257458219202824\n",
            "train loss:2.2321325682424003\n",
            "=== epoch:47, train acc:0.23, test acc:0.2134 ===\n",
            "train loss:2.2092072912656855\n",
            "train loss:2.2328413153181885\n",
            "train loss:2.2340579298311916\n",
            "=== epoch:48, train acc:0.23666666666666666, test acc:0.2176 ===\n",
            "train loss:2.229279220030772\n",
            "train loss:2.19553224268808\n",
            "train loss:2.2218203322907915\n",
            "=== epoch:49, train acc:0.24333333333333335, test acc:0.2195 ===\n",
            "train loss:2.224418929413781\n",
            "train loss:2.1693420009764885\n",
            "train loss:2.1916524750182385\n",
            "=== epoch:50, train acc:0.23666666666666666, test acc:0.2129 ===\n",
            "train loss:2.2110318239993627\n",
            "train loss:2.208827809391516\n",
            "train loss:2.2172644551400116\n",
            "=== epoch:51, train acc:0.24, test acc:0.2134 ===\n",
            "train loss:2.2010037168525507\n",
            "train loss:2.2120676644900823\n",
            "train loss:2.1479015778157695\n",
            "=== epoch:52, train acc:0.23666666666666666, test acc:0.2065 ===\n",
            "train loss:2.2342687469453466\n",
            "train loss:2.228433294021239\n",
            "train loss:2.195894073806688\n",
            "=== epoch:53, train acc:0.24, test acc:0.2102 ===\n",
            "train loss:2.2285078352722634\n",
            "train loss:2.201378265425738\n",
            "train loss:2.2205193680248843\n",
            "=== epoch:54, train acc:0.24333333333333335, test acc:0.2106 ===\n",
            "train loss:2.2239692958828847\n",
            "train loss:2.179185786983216\n",
            "train loss:2.1989715895021846\n",
            "=== epoch:55, train acc:0.24, test acc:0.2068 ===\n",
            "train loss:2.175319705014236\n",
            "train loss:2.2305011532304952\n",
            "train loss:2.206988973868994\n",
            "=== epoch:56, train acc:0.24, test acc:0.211 ===\n",
            "train loss:2.2076005200367113\n",
            "train loss:2.2069080525925657\n",
            "train loss:2.218772469504684\n",
            "=== epoch:57, train acc:0.24666666666666667, test acc:0.2151 ===\n",
            "train loss:2.2003943573108113\n",
            "train loss:2.187462489074449\n",
            "train loss:2.201992597052658\n",
            "=== epoch:58, train acc:0.24666666666666667, test acc:0.2178 ===\n",
            "train loss:2.2542794288586196\n",
            "train loss:2.1688828137901273\n",
            "train loss:2.164338156463471\n",
            "=== epoch:59, train acc:0.25, test acc:0.2162 ===\n",
            "train loss:2.1826713079945135\n",
            "train loss:2.206715244870878\n",
            "train loss:2.1555026970758484\n",
            "=== epoch:60, train acc:0.25, test acc:0.2162 ===\n",
            "train loss:2.1666633227073917\n",
            "train loss:2.235985342148772\n",
            "train loss:2.2289234429129863\n",
            "=== epoch:61, train acc:0.25666666666666665, test acc:0.2233 ===\n",
            "train loss:2.1629329916913926\n",
            "train loss:2.196681182693049\n",
            "train loss:2.161376803872113\n",
            "=== epoch:62, train acc:0.25666666666666665, test acc:0.2252 ===\n",
            "train loss:2.204884628665255\n",
            "train loss:2.1976572882300616\n",
            "train loss:2.1891715785877266\n",
            "=== epoch:63, train acc:0.2633333333333333, test acc:0.2288 ===\n",
            "train loss:2.186219207715139\n",
            "train loss:2.185656308368679\n",
            "train loss:2.166164179037618\n",
            "=== epoch:64, train acc:0.26, test acc:0.2261 ===\n",
            "train loss:2.2154125395434754\n",
            "train loss:2.1733414602473493\n",
            "train loss:2.1778772065330263\n",
            "=== epoch:65, train acc:0.26666666666666666, test acc:0.229 ===\n",
            "train loss:2.2245620101655934\n",
            "train loss:2.145404558522707\n",
            "train loss:2.1779590441118533\n",
            "=== epoch:66, train acc:0.27, test acc:0.2316 ===\n",
            "train loss:2.197106165592312\n",
            "train loss:2.178265414608939\n",
            "train loss:2.182817086245903\n",
            "=== epoch:67, train acc:0.2733333333333333, test acc:0.2345 ===\n",
            "train loss:2.1611743516026207\n",
            "train loss:2.1484765469121827\n",
            "train loss:2.169360770172872\n",
            "=== epoch:68, train acc:0.27666666666666667, test acc:0.2351 ===\n",
            "train loss:2.182296415460398\n",
            "train loss:2.198240552280553\n",
            "train loss:2.179531651466757\n",
            "=== epoch:69, train acc:0.28, test acc:0.2385 ===\n",
            "train loss:2.1438668346556757\n",
            "train loss:2.215588820280895\n",
            "train loss:2.1306190656310755\n",
            "=== epoch:70, train acc:0.2833333333333333, test acc:0.2434 ===\n",
            "train loss:2.1650510789244373\n",
            "train loss:2.169986049278599\n",
            "train loss:2.142852086123989\n",
            "=== epoch:71, train acc:0.2866666666666667, test acc:0.2466 ===\n",
            "train loss:2.1659049701871878\n",
            "train loss:2.127351816455036\n",
            "train loss:2.1518123064266956\n",
            "=== epoch:72, train acc:0.2866666666666667, test acc:0.2451 ===\n",
            "train loss:2.1806066738853973\n",
            "train loss:2.239179414931921\n",
            "train loss:2.1822526530676174\n",
            "=== epoch:73, train acc:0.29, test acc:0.2534 ===\n",
            "train loss:2.2023877449155327\n",
            "train loss:2.1225646423273474\n",
            "train loss:2.1350777319479857\n",
            "=== epoch:74, train acc:0.29, test acc:0.2537 ===\n",
            "train loss:2.2073717216433573\n",
            "train loss:2.159419942667158\n",
            "train loss:2.1708405898493\n",
            "=== epoch:75, train acc:0.30333333333333334, test acc:0.2586 ===\n",
            "train loss:2.166319052521296\n",
            "train loss:2.126256447738565\n",
            "train loss:2.189953679654236\n",
            "=== epoch:76, train acc:0.30666666666666664, test acc:0.2601 ===\n",
            "train loss:2.165404619742907\n",
            "train loss:2.1132074649913886\n",
            "train loss:2.17061976287204\n",
            "=== epoch:77, train acc:0.30666666666666664, test acc:0.2608 ===\n",
            "train loss:2.1428234007222398\n",
            "train loss:2.0697726034173787\n",
            "train loss:2.1373125161959203\n",
            "=== epoch:78, train acc:0.30333333333333334, test acc:0.2589 ===\n",
            "train loss:2.0791675428420886\n",
            "train loss:2.0717541328274796\n",
            "train loss:2.1073393578846797\n",
            "=== epoch:79, train acc:0.30666666666666664, test acc:0.2563 ===\n",
            "train loss:2.134265652588713\n",
            "train loss:2.2105958163473547\n",
            "train loss:2.1619198280066616\n",
            "=== epoch:80, train acc:0.31333333333333335, test acc:0.2637 ===\n",
            "train loss:2.1229689074488785\n",
            "train loss:2.144452056903363\n",
            "train loss:2.0914503284204256\n",
            "=== epoch:81, train acc:0.31, test acc:0.2637 ===\n",
            "train loss:2.1550203366582714\n",
            "train loss:2.1069356907384442\n",
            "train loss:2.1199976791628448\n",
            "=== epoch:82, train acc:0.31666666666666665, test acc:0.2669 ===\n",
            "train loss:2.179291558074559\n",
            "train loss:2.0969550016448344\n",
            "train loss:2.10780725031769\n",
            "=== epoch:83, train acc:0.32666666666666666, test acc:0.2718 ===\n",
            "train loss:2.0508865431894687\n",
            "train loss:2.179845381299178\n",
            "train loss:2.1304523662897252\n",
            "=== epoch:84, train acc:0.33, test acc:0.2715 ===\n",
            "train loss:2.1061525300372166\n",
            "train loss:2.1122505155337934\n",
            "train loss:2.1050702134967647\n",
            "=== epoch:85, train acc:0.3233333333333333, test acc:0.2709 ===\n",
            "train loss:2.0928946318477886\n",
            "train loss:2.0903059105845085\n",
            "train loss:2.041416556018753\n",
            "=== epoch:86, train acc:0.32666666666666666, test acc:0.2725 ===\n",
            "train loss:2.0987272170821965\n",
            "train loss:2.098878436481739\n",
            "train loss:2.142840444606664\n",
            "=== epoch:87, train acc:0.31666666666666665, test acc:0.2702 ===\n",
            "train loss:2.1893362273224466\n",
            "train loss:2.0306277720608854\n",
            "train loss:2.045017923210094\n",
            "=== epoch:88, train acc:0.3233333333333333, test acc:0.272 ===\n",
            "train loss:2.1306462978586094\n",
            "train loss:2.104668567246851\n",
            "train loss:2.046227425958363\n",
            "=== epoch:89, train acc:0.3233333333333333, test acc:0.2726 ===\n",
            "train loss:2.1286513054692686\n",
            "train loss:2.0926915661652274\n",
            "train loss:2.08815392939546\n",
            "=== epoch:90, train acc:0.3333333333333333, test acc:0.275 ===\n",
            "train loss:2.086432858772647\n",
            "train loss:2.10474895058116\n",
            "train loss:2.0849398763366667\n",
            "=== epoch:91, train acc:0.33, test acc:0.2741 ===\n",
            "train loss:2.0638166623368988\n",
            "train loss:2.1005845579656675\n",
            "train loss:2.0646739687761686\n",
            "=== epoch:92, train acc:0.33, test acc:0.2741 ===\n",
            "train loss:2.1657486295613277\n",
            "train loss:2.128266690525606\n",
            "train loss:1.9736847960532413\n",
            "=== epoch:93, train acc:0.3333333333333333, test acc:0.2738 ===\n",
            "train loss:2.0706906339450795\n",
            "train loss:2.0362821571488623\n",
            "train loss:2.0799429251143575\n",
            "=== epoch:94, train acc:0.3333333333333333, test acc:0.2729 ===\n",
            "train loss:2.081014123450462\n",
            "train loss:2.0522344657869374\n",
            "train loss:2.0391658853611663\n",
            "=== epoch:95, train acc:0.34, test acc:0.2765 ===\n",
            "train loss:2.038058044865825\n",
            "train loss:2.038301046423163\n",
            "train loss:2.0178883015277727\n",
            "=== epoch:96, train acc:0.35333333333333333, test acc:0.2776 ===\n",
            "train loss:2.0678233482074786\n",
            "train loss:2.1368235970032314\n",
            "train loss:2.030668481575354\n",
            "=== epoch:97, train acc:0.3566666666666667, test acc:0.281 ===\n",
            "train loss:2.1191748245221884\n",
            "train loss:2.0647392944771177\n",
            "train loss:2.0438399554805273\n",
            "=== epoch:98, train acc:0.36, test acc:0.2842 ===\n",
            "train loss:2.009191670241149\n",
            "train loss:2.038997807512745\n",
            "train loss:2.0840415213288845\n",
            "=== epoch:99, train acc:0.36, test acc:0.282 ===\n",
            "train loss:1.9399231838510689\n",
            "train loss:1.9644350021773886\n",
            "train loss:2.1168233043929168\n",
            "=== epoch:100, train acc:0.36, test acc:0.2827 ===\n",
            "train loss:2.0422774910303274\n",
            "train loss:2.016620079104579\n",
            "train loss:2.065290736816533\n",
            "=== epoch:101, train acc:0.35, test acc:0.2822 ===\n",
            "train loss:2.148595907059817\n",
            "train loss:2.076817655854819\n",
            "train loss:2.0209874134079517\n",
            "=== epoch:102, train acc:0.3566666666666667, test acc:0.2851 ===\n",
            "train loss:1.9863069400330793\n",
            "train loss:2.0116833035300545\n",
            "train loss:2.062129442279617\n",
            "=== epoch:103, train acc:0.35333333333333333, test acc:0.2855 ===\n",
            "train loss:1.9924080885286863\n",
            "train loss:2.0531451484504966\n",
            "train loss:2.068677731078181\n",
            "=== epoch:104, train acc:0.35333333333333333, test acc:0.2884 ===\n",
            "train loss:1.951837777770144\n",
            "train loss:1.969257077153346\n",
            "train loss:2.104906865118526\n",
            "=== epoch:105, train acc:0.35333333333333333, test acc:0.287 ===\n",
            "train loss:2.0533730206179417\n",
            "train loss:2.0298428615879405\n",
            "train loss:2.021885728675396\n",
            "=== epoch:106, train acc:0.3566666666666667, test acc:0.2874 ===\n",
            "train loss:2.1142447160781956\n",
            "train loss:2.081071970449983\n",
            "train loss:1.987969043024818\n",
            "=== epoch:107, train acc:0.36333333333333334, test acc:0.295 ===\n",
            "train loss:2.1020639866495343\n",
            "train loss:1.9979457417338937\n",
            "train loss:2.098151729447769\n",
            "=== epoch:108, train acc:0.3566666666666667, test acc:0.2897 ===\n",
            "train loss:1.9168590610740528\n",
            "train loss:1.9696951411592412\n",
            "train loss:2.0001933071408784\n",
            "=== epoch:109, train acc:0.3466666666666667, test acc:0.285 ===\n",
            "train loss:2.0142984608701364\n",
            "train loss:2.068697092038339\n",
            "train loss:1.9869563161242538\n",
            "=== epoch:110, train acc:0.35333333333333333, test acc:0.2869 ===\n",
            "train loss:1.9573179314815663\n",
            "train loss:2.0438856463317454\n",
            "train loss:2.1026300267334737\n",
            "=== epoch:111, train acc:0.3566666666666667, test acc:0.2896 ===\n",
            "train loss:2.008719634401267\n",
            "train loss:2.0567009877662685\n",
            "train loss:2.06444678762284\n",
            "=== epoch:112, train acc:0.3566666666666667, test acc:0.2872 ===\n",
            "train loss:2.0383968209944583\n",
            "train loss:1.9937629153376724\n",
            "train loss:2.0225534746239617\n",
            "=== epoch:113, train acc:0.36, test acc:0.2895 ===\n",
            "train loss:1.9401729347655543\n",
            "train loss:2.014846382924262\n",
            "train loss:2.0185501283289895\n",
            "=== epoch:114, train acc:0.35333333333333333, test acc:0.2854 ===\n",
            "train loss:2.0845255877060564\n",
            "train loss:2.001957421647596\n",
            "train loss:1.9889002449902113\n",
            "=== epoch:115, train acc:0.35333333333333333, test acc:0.2828 ===\n",
            "train loss:1.9999573368960628\n",
            "train loss:2.0075747032278266\n",
            "train loss:1.9653721156209991\n",
            "=== epoch:116, train acc:0.35333333333333333, test acc:0.2808 ===\n",
            "train loss:1.9613389211333356\n",
            "train loss:1.956941776136156\n",
            "train loss:2.025095103788881\n",
            "=== epoch:117, train acc:0.3566666666666667, test acc:0.281 ===\n",
            "train loss:1.9549006336640864\n",
            "train loss:1.8715275567497909\n",
            "train loss:1.9435347601398456\n",
            "=== epoch:118, train acc:0.36, test acc:0.2776 ===\n",
            "train loss:2.0985848140783263\n",
            "train loss:1.9128889337653177\n",
            "train loss:2.029009325468447\n",
            "=== epoch:119, train acc:0.35, test acc:0.2756 ===\n",
            "train loss:1.9726076045996295\n",
            "train loss:1.992438295035544\n",
            "train loss:1.9273857358947912\n",
            "=== epoch:120, train acc:0.35, test acc:0.2733 ===\n",
            "train loss:1.9709910128243158\n",
            "train loss:1.9275528899342296\n",
            "train loss:1.8704743252074567\n",
            "=== epoch:121, train acc:0.3466666666666667, test acc:0.2745 ===\n",
            "train loss:1.918532383697099\n",
            "train loss:2.023316378171756\n",
            "train loss:1.907110701310587\n",
            "=== epoch:122, train acc:0.3566666666666667, test acc:0.2797 ===\n",
            "train loss:1.9560056291244343\n",
            "train loss:1.9985741309961935\n",
            "train loss:1.8641239999781407\n",
            "=== epoch:123, train acc:0.3466666666666667, test acc:0.2723 ===\n",
            "train loss:1.9517032019065765\n",
            "train loss:1.9681065174690704\n",
            "train loss:1.96624874556602\n",
            "=== epoch:124, train acc:0.36, test acc:0.2784 ===\n",
            "train loss:1.962400226903316\n",
            "train loss:1.917634413256792\n",
            "train loss:1.9275564570040347\n",
            "=== epoch:125, train acc:0.36, test acc:0.2837 ===\n",
            "train loss:1.9496366644180967\n",
            "train loss:1.9426525809270927\n",
            "train loss:1.944602176757514\n",
            "=== epoch:126, train acc:0.36, test acc:0.2909 ===\n",
            "train loss:1.9937653845429903\n",
            "train loss:2.011779733684264\n",
            "train loss:1.9645387748960004\n",
            "=== epoch:127, train acc:0.36666666666666664, test acc:0.2941 ===\n",
            "train loss:1.9494034667126772\n",
            "train loss:1.9298789466175474\n",
            "train loss:2.0153032786237683\n",
            "=== epoch:128, train acc:0.37333333333333335, test acc:0.2913 ===\n",
            "train loss:2.0145654983720744\n",
            "train loss:1.8357507196535372\n",
            "train loss:2.0357038984047393\n",
            "=== epoch:129, train acc:0.37333333333333335, test acc:0.2953 ===\n",
            "train loss:1.9653398024117934\n",
            "train loss:1.9267891020699082\n",
            "train loss:1.8897110374285289\n",
            "=== epoch:130, train acc:0.37333333333333335, test acc:0.2995 ===\n",
            "train loss:1.9359247245089803\n",
            "train loss:1.8981309922373402\n",
            "train loss:1.96215718638206\n",
            "=== epoch:131, train acc:0.37666666666666665, test acc:0.2936 ===\n",
            "train loss:1.8790076242803702\n",
            "train loss:1.9616828693318338\n",
            "train loss:1.898836250095876\n",
            "=== epoch:132, train acc:0.37333333333333335, test acc:0.2994 ===\n",
            "train loss:1.9533959204732705\n",
            "train loss:1.9820250452139956\n",
            "train loss:1.92598903550752\n",
            "=== epoch:133, train acc:0.38333333333333336, test acc:0.3045 ===\n",
            "train loss:1.9313078925309404\n",
            "train loss:1.855379674493542\n",
            "train loss:1.9110298012808289\n",
            "=== epoch:134, train acc:0.3933333333333333, test acc:0.3049 ===\n",
            "train loss:1.9469531300382439\n",
            "train loss:2.02252311382664\n",
            "train loss:1.8681071047932993\n",
            "=== epoch:135, train acc:0.39666666666666667, test acc:0.3035 ===\n",
            "train loss:1.9838492314296574\n",
            "train loss:1.920238275073239\n",
            "train loss:1.9397787084733462\n",
            "=== epoch:136, train acc:0.4033333333333333, test acc:0.3039 ===\n",
            "train loss:2.020929204533526\n",
            "train loss:1.8869898243201255\n",
            "train loss:1.928502661835827\n",
            "=== epoch:137, train acc:0.4066666666666667, test acc:0.3063 ===\n",
            "train loss:1.9493474268515671\n",
            "train loss:1.9134254250242537\n",
            "train loss:1.9507122117888915\n",
            "=== epoch:138, train acc:0.4033333333333333, test acc:0.308 ===\n",
            "train loss:1.9089291912847661\n",
            "train loss:1.9635923154946737\n",
            "train loss:2.0099511295443837\n",
            "=== epoch:139, train acc:0.4, test acc:0.3063 ===\n",
            "train loss:1.8702879106263586\n",
            "train loss:1.9724825496788174\n",
            "train loss:1.8744773372389398\n",
            "=== epoch:140, train acc:0.4066666666666667, test acc:0.313 ===\n",
            "train loss:2.0031009595683185\n",
            "train loss:1.890316840610866\n",
            "train loss:1.8563846309431267\n",
            "=== epoch:141, train acc:0.41333333333333333, test acc:0.3124 ===\n",
            "train loss:1.9518268581932876\n",
            "train loss:1.9379444991357682\n",
            "train loss:1.9856609355723915\n",
            "=== epoch:142, train acc:0.41333333333333333, test acc:0.319 ===\n",
            "train loss:2.009475871892938\n",
            "train loss:1.8179489717752053\n",
            "train loss:1.8650320401360947\n",
            "=== epoch:143, train acc:0.41, test acc:0.3173 ===\n",
            "train loss:1.7998939303062342\n",
            "train loss:1.9201748670607506\n",
            "train loss:1.8558830909640665\n",
            "=== epoch:144, train acc:0.41, test acc:0.3171 ===\n",
            "train loss:1.90751195918956\n",
            "train loss:1.8628514763278041\n",
            "train loss:1.8851331835312723\n",
            "=== epoch:145, train acc:0.41, test acc:0.32 ===\n",
            "train loss:1.9923685270473095\n",
            "train loss:1.8511612844233778\n",
            "train loss:1.7848774859917915\n",
            "=== epoch:146, train acc:0.4066666666666667, test acc:0.3161 ===\n",
            "train loss:1.7972805547923683\n",
            "train loss:1.7855596060569254\n",
            "train loss:1.8150790705258097\n",
            "=== epoch:147, train acc:0.4033333333333333, test acc:0.306 ===\n",
            "train loss:1.9605042967385256\n",
            "train loss:1.9596082897744123\n",
            "train loss:1.9266438732078612\n",
            "=== epoch:148, train acc:0.39666666666666667, test acc:0.3088 ===\n",
            "train loss:1.8597375033948378\n",
            "train loss:1.9096109444050475\n",
            "train loss:1.7164655432805471\n",
            "=== epoch:149, train acc:0.4, test acc:0.3069 ===\n",
            "train loss:1.7963717407771371\n",
            "train loss:1.7991091228053588\n",
            "train loss:1.836362103645942\n",
            "=== epoch:150, train acc:0.39, test acc:0.3068 ===\n",
            "train loss:1.9653640707805677\n",
            "train loss:1.9193710013133163\n",
            "train loss:1.8307577097379564\n",
            "=== epoch:151, train acc:0.39, test acc:0.3064 ===\n",
            "train loss:1.9145580900368122\n",
            "train loss:1.9089500722433614\n",
            "train loss:1.9612819560830914\n",
            "=== epoch:152, train acc:0.4033333333333333, test acc:0.3087 ===\n",
            "train loss:1.9286743323021887\n",
            "train loss:1.8446960014024334\n",
            "train loss:1.8725002743596653\n",
            "=== epoch:153, train acc:0.4033333333333333, test acc:0.3044 ===\n",
            "train loss:1.774735661095218\n",
            "train loss:1.860191743235637\n",
            "train loss:1.816264948849095\n",
            "=== epoch:154, train acc:0.4066666666666667, test acc:0.3048 ===\n",
            "train loss:1.952390707889155\n",
            "train loss:1.8969862193042064\n",
            "train loss:1.848695282540845\n",
            "=== epoch:155, train acc:0.41, test acc:0.3087 ===\n",
            "train loss:1.9052731361298547\n",
            "train loss:1.7805383304142608\n",
            "train loss:1.8606963221405968\n",
            "=== epoch:156, train acc:0.39666666666666667, test acc:0.3065 ===\n",
            "train loss:1.837140149387208\n",
            "train loss:1.864910394281056\n",
            "train loss:1.7902649507052733\n",
            "=== epoch:157, train acc:0.4066666666666667, test acc:0.3118 ===\n",
            "train loss:1.8043284147003016\n",
            "train loss:1.789747714675554\n",
            "train loss:1.9414891645467645\n",
            "=== epoch:158, train acc:0.4166666666666667, test acc:0.3188 ===\n",
            "train loss:1.8721119520230014\n",
            "train loss:1.8782670954569096\n",
            "train loss:1.7335963250450397\n",
            "=== epoch:159, train acc:0.41333333333333333, test acc:0.3171 ===\n",
            "train loss:1.875813067448877\n",
            "train loss:1.7742240509570792\n",
            "train loss:1.8001142447955945\n",
            "=== epoch:160, train acc:0.43, test acc:0.3216 ===\n",
            "train loss:1.9436039931332858\n",
            "train loss:1.7896647946531772\n",
            "train loss:1.7905557719728247\n",
            "=== epoch:161, train acc:0.43333333333333335, test acc:0.3289 ===\n",
            "train loss:1.8458749742110085\n",
            "train loss:1.9184370557538957\n",
            "train loss:1.6795614334671947\n",
            "=== epoch:162, train acc:0.43666666666666665, test acc:0.3355 ===\n",
            "train loss:1.7546835507842615\n",
            "train loss:1.8342017412554872\n",
            "train loss:1.7689678007210246\n",
            "=== epoch:163, train acc:0.44, test acc:0.3411 ===\n",
            "train loss:1.8517335245721172\n",
            "train loss:1.8420715184995553\n",
            "train loss:1.7671811524985166\n",
            "=== epoch:164, train acc:0.44666666666666666, test acc:0.3424 ===\n",
            "train loss:1.8521440351301839\n",
            "train loss:1.8171353084502815\n",
            "train loss:1.724942599155954\n",
            "=== epoch:165, train acc:0.44666666666666666, test acc:0.3407 ===\n",
            "train loss:1.8114988598997561\n",
            "train loss:1.71314134710866\n",
            "train loss:1.8964709015208536\n",
            "=== epoch:166, train acc:0.4533333333333333, test acc:0.3433 ===\n",
            "train loss:1.852616753606016\n",
            "train loss:1.7799882371793763\n",
            "train loss:1.691889650538555\n",
            "=== epoch:167, train acc:0.44666666666666666, test acc:0.3376 ===\n",
            "train loss:1.717823777862922\n",
            "train loss:1.7878039507008219\n",
            "train loss:1.795680485438345\n",
            "=== epoch:168, train acc:0.45, test acc:0.3423 ===\n",
            "train loss:1.8499122847521718\n",
            "train loss:1.7915283984486796\n",
            "train loss:1.6853923961086923\n",
            "=== epoch:169, train acc:0.4533333333333333, test acc:0.3437 ===\n",
            "train loss:1.8374309451003967\n",
            "train loss:1.7612740131978581\n",
            "train loss:1.7178401750378416\n",
            "=== epoch:170, train acc:0.45, test acc:0.3427 ===\n",
            "train loss:1.7381609134566494\n",
            "train loss:1.7615384009503448\n",
            "train loss:1.785313395184093\n",
            "=== epoch:171, train acc:0.4533333333333333, test acc:0.3466 ===\n",
            "train loss:1.7402273472187522\n",
            "train loss:1.7817887757905146\n",
            "train loss:1.6537943945090994\n",
            "=== epoch:172, train acc:0.44666666666666666, test acc:0.3501 ===\n",
            "train loss:1.7990011163771107\n",
            "train loss:1.7745970850002275\n",
            "train loss:1.7638681726146204\n",
            "=== epoch:173, train acc:0.45, test acc:0.3525 ===\n",
            "train loss:1.8654617269132705\n",
            "train loss:1.7020283271706307\n",
            "train loss:1.8258259164107933\n",
            "=== epoch:174, train acc:0.46, test acc:0.3553 ===\n",
            "train loss:1.8254178645772894\n",
            "train loss:1.8961966902690457\n",
            "train loss:1.7518356591248938\n",
            "=== epoch:175, train acc:0.4533333333333333, test acc:0.3539 ===\n",
            "train loss:1.8409985224149412\n",
            "train loss:1.6219628906762948\n",
            "train loss:1.8258716962969574\n",
            "=== epoch:176, train acc:0.4633333333333333, test acc:0.3542 ===\n",
            "train loss:1.763132493345652\n",
            "train loss:1.9494341351468623\n",
            "train loss:1.7321579700929806\n",
            "=== epoch:177, train acc:0.46, test acc:0.3551 ===\n",
            "train loss:1.6939214114686567\n",
            "train loss:1.7786751908880265\n",
            "train loss:1.7408169760958316\n",
            "=== epoch:178, train acc:0.45666666666666667, test acc:0.3576 ===\n",
            "train loss:1.6542896182777238\n",
            "train loss:1.856175450878461\n",
            "train loss:1.8264492908194443\n",
            "=== epoch:179, train acc:0.46, test acc:0.3605 ===\n",
            "train loss:1.7954290439742295\n",
            "train loss:1.561576649569808\n",
            "train loss:1.7292295006747531\n",
            "=== epoch:180, train acc:0.4533333333333333, test acc:0.3554 ===\n",
            "train loss:1.6947506069005505\n",
            "train loss:1.5840155449511177\n",
            "train loss:1.8085079763355654\n",
            "=== epoch:181, train acc:0.46, test acc:0.3564 ===\n",
            "train loss:1.5485899899869195\n",
            "train loss:1.6790654737860617\n",
            "train loss:1.662379594486831\n",
            "=== epoch:182, train acc:0.46, test acc:0.3548 ===\n",
            "train loss:1.6861782242919636\n",
            "train loss:1.7256149741913125\n",
            "train loss:1.6755515525013884\n",
            "=== epoch:183, train acc:0.4666666666666667, test acc:0.3604 ===\n",
            "train loss:1.6646322921717789\n",
            "train loss:1.856416095794056\n",
            "train loss:1.8050668416876494\n",
            "=== epoch:184, train acc:0.4666666666666667, test acc:0.3623 ===\n",
            "train loss:1.687008846637996\n",
            "train loss:1.8570656093674658\n",
            "train loss:1.7278921171686294\n",
            "=== epoch:185, train acc:0.47333333333333333, test acc:0.3698 ===\n",
            "train loss:1.6027435258228102\n",
            "train loss:1.7932950520104816\n",
            "train loss:1.6834998402871406\n",
            "=== epoch:186, train acc:0.4766666666666667, test acc:0.372 ===\n",
            "train loss:1.6464288005885899\n",
            "train loss:1.6793961062486138\n",
            "train loss:1.707780625503747\n",
            "=== epoch:187, train acc:0.48333333333333334, test acc:0.3722 ===\n",
            "train loss:1.8350095743898953\n",
            "train loss:1.5917007016846796\n",
            "train loss:1.7563929339308433\n",
            "=== epoch:188, train acc:0.48, test acc:0.375 ===\n",
            "train loss:1.7793613990844603\n",
            "train loss:1.9409711911074714\n",
            "train loss:1.7460067679946025\n",
            "=== epoch:189, train acc:0.4866666666666667, test acc:0.3797 ===\n",
            "train loss:1.6127092414507038\n",
            "train loss:1.7273459720091926\n",
            "train loss:1.7081594377279294\n",
            "=== epoch:190, train acc:0.4866666666666667, test acc:0.3813 ===\n",
            "train loss:1.8087731933191378\n",
            "train loss:1.7276212871019305\n",
            "train loss:1.713021191593225\n",
            "=== epoch:191, train acc:0.49, test acc:0.3816 ===\n",
            "train loss:1.696954512921841\n",
            "train loss:1.6935287799415617\n",
            "train loss:1.6786344112832623\n",
            "=== epoch:192, train acc:0.49666666666666665, test acc:0.3841 ===\n",
            "train loss:1.6765028735811227\n",
            "train loss:1.6550818982707651\n",
            "train loss:1.691108407029895\n",
            "=== epoch:193, train acc:0.5, test acc:0.3857 ===\n",
            "train loss:1.8256282834486468\n",
            "train loss:1.772094646625958\n",
            "train loss:1.7199644283166322\n",
            "=== epoch:194, train acc:0.5066666666666667, test acc:0.3906 ===\n",
            "train loss:1.7182646055878705\n",
            "train loss:1.6790830253047182\n",
            "train loss:1.7072755493802076\n",
            "=== epoch:195, train acc:0.5066666666666667, test acc:0.3932 ===\n",
            "train loss:1.5457960374977284\n",
            "train loss:1.7238028111857218\n",
            "train loss:1.665095622648559\n",
            "=== epoch:196, train acc:0.5066666666666667, test acc:0.3914 ===\n",
            "train loss:1.6823996105399006\n",
            "train loss:1.6777053911127724\n",
            "train loss:1.655482362167293\n",
            "=== epoch:197, train acc:0.5066666666666667, test acc:0.3924 ===\n",
            "train loss:1.713589193788101\n",
            "train loss:1.671245521023566\n",
            "train loss:1.7062798552875342\n",
            "=== epoch:198, train acc:0.5066666666666667, test acc:0.395 ===\n",
            "train loss:1.7070176223151052\n",
            "train loss:1.6347187730775696\n",
            "train loss:1.7109562957485218\n",
            "=== epoch:199, train acc:0.51, test acc:0.3953 ===\n",
            "train loss:1.5698665596131065\n",
            "train loss:1.6818285362685799\n",
            "train loss:1.6033373295627584\n",
            "=== epoch:200, train acc:0.51, test acc:0.3956 ===\n",
            "train loss:1.6524348463234246\n",
            "train loss:1.5832142378246241\n",
            "train loss:1.5985263438797772\n",
            "=== epoch:201, train acc:0.5166666666666667, test acc:0.3986 ===\n",
            "train loss:1.7298287787061108\n",
            "train loss:1.5762240250231057\n",
            "train loss:1.730912669581101\n",
            "=== epoch:202, train acc:0.52, test acc:0.4022 ===\n",
            "train loss:1.617406776340443\n",
            "train loss:1.6963020705906229\n",
            "train loss:1.6119754173000829\n",
            "=== epoch:203, train acc:0.52, test acc:0.4009 ===\n",
            "train loss:1.7163841433352445\n",
            "train loss:1.6099780398122328\n",
            "train loss:1.6215574729922664\n",
            "=== epoch:204, train acc:0.5233333333333333, test acc:0.4038 ===\n",
            "train loss:1.621684033020581\n",
            "train loss:1.632826303147597\n",
            "train loss:1.532485076809591\n",
            "=== epoch:205, train acc:0.5266666666666666, test acc:0.4038 ===\n",
            "train loss:1.5652737638639909\n",
            "train loss:1.7043312147368213\n",
            "train loss:1.6181192354001928\n",
            "=== epoch:206, train acc:0.5266666666666666, test acc:0.4039 ===\n",
            "train loss:1.6478903026829377\n",
            "train loss:1.5958039580517434\n",
            "train loss:1.6372338274996405\n",
            "=== epoch:207, train acc:0.53, test acc:0.4046 ===\n",
            "train loss:1.6318334479506351\n",
            "train loss:1.6137641253305435\n",
            "train loss:1.5230664863428403\n",
            "=== epoch:208, train acc:0.5266666666666666, test acc:0.4081 ===\n",
            "train loss:1.6141406744137567\n",
            "train loss:1.4922850379966273\n",
            "train loss:1.5546487025721032\n",
            "=== epoch:209, train acc:0.5266666666666666, test acc:0.4082 ===\n",
            "train loss:1.5991167823461663\n",
            "train loss:1.6552990649583643\n",
            "train loss:1.5945294710553863\n",
            "=== epoch:210, train acc:0.5266666666666666, test acc:0.4105 ===\n",
            "train loss:1.612980496796392\n",
            "train loss:1.6424046671756218\n",
            "train loss:1.6492537483786136\n",
            "=== epoch:211, train acc:0.5266666666666666, test acc:0.4096 ===\n",
            "train loss:1.5584035820238389\n",
            "train loss:1.5924367818584806\n",
            "train loss:1.5826529495988413\n",
            "=== epoch:212, train acc:0.5333333333333333, test acc:0.41 ===\n",
            "train loss:1.5269616407394895\n",
            "train loss:1.6271683411964712\n",
            "train loss:1.5011941179190882\n",
            "=== epoch:213, train acc:0.5333333333333333, test acc:0.4104 ===\n",
            "train loss:1.6268716121959466\n",
            "train loss:1.5816685254785114\n",
            "train loss:1.6579064977139848\n",
            "=== epoch:214, train acc:0.5366666666666666, test acc:0.4143 ===\n",
            "train loss:1.543762817622751\n",
            "train loss:1.6008162092305305\n",
            "train loss:1.4009459195285765\n",
            "=== epoch:215, train acc:0.5433333333333333, test acc:0.4125 ===\n",
            "train loss:1.6842213653016065\n",
            "train loss:1.6004196096728185\n",
            "train loss:1.5747410680342528\n",
            "=== epoch:216, train acc:0.5366666666666666, test acc:0.4148 ===\n",
            "train loss:1.4276257067176263\n",
            "train loss:1.553939052461962\n",
            "train loss:1.5488300386745737\n",
            "=== epoch:217, train acc:0.5433333333333333, test acc:0.4181 ===\n",
            "train loss:1.528091201684805\n",
            "train loss:1.509770707794736\n",
            "train loss:1.5247610246504018\n",
            "=== epoch:218, train acc:0.54, test acc:0.4177 ===\n",
            "train loss:1.3746994870055034\n",
            "train loss:1.6536520765746026\n",
            "train loss:1.3453147530099168\n",
            "=== epoch:219, train acc:0.5366666666666666, test acc:0.4194 ===\n",
            "train loss:1.4247514236775616\n",
            "train loss:1.4768045406524353\n",
            "train loss:1.4577949774068568\n",
            "=== epoch:220, train acc:0.5466666666666666, test acc:0.4219 ===\n",
            "train loss:1.512734522251184\n",
            "train loss:1.5662819980646956\n",
            "train loss:1.462974735635354\n",
            "=== epoch:221, train acc:0.5666666666666667, test acc:0.4189 ===\n",
            "train loss:1.583985111370957\n",
            "train loss:1.4208834791486333\n",
            "train loss:1.3930217766005617\n",
            "=== epoch:222, train acc:0.5533333333333333, test acc:0.4217 ===\n",
            "train loss:1.3201509995983562\n",
            "train loss:1.5174606735740557\n",
            "train loss:1.4947557372212872\n",
            "=== epoch:223, train acc:0.5666666666666667, test acc:0.4201 ===\n",
            "train loss:1.5291398191767382\n",
            "train loss:1.6352967342180655\n",
            "train loss:1.4771744719469486\n",
            "=== epoch:224, train acc:0.56, test acc:0.4218 ===\n",
            "train loss:1.4164396532644759\n",
            "train loss:1.5141742532149443\n",
            "train loss:1.4432399923746664\n",
            "=== epoch:225, train acc:0.5566666666666666, test acc:0.4199 ===\n",
            "train loss:1.3331864948587444\n",
            "train loss:1.4895808315565995\n",
            "train loss:1.3820973471672051\n",
            "=== epoch:226, train acc:0.5566666666666666, test acc:0.42 ===\n",
            "train loss:1.4550482055089282\n",
            "train loss:1.4927817410435125\n",
            "train loss:1.578512812424836\n",
            "=== epoch:227, train acc:0.57, test acc:0.4188 ===\n",
            "train loss:1.5251747538551463\n",
            "train loss:1.5438415267491532\n",
            "train loss:1.5396869442725014\n",
            "=== epoch:228, train acc:0.57, test acc:0.4212 ===\n",
            "train loss:1.553775818951324\n",
            "train loss:1.4388745142471513\n",
            "train loss:1.4185162790879111\n",
            "=== epoch:229, train acc:0.5766666666666667, test acc:0.423 ===\n",
            "train loss:1.488998336646546\n",
            "train loss:1.3259855499519182\n",
            "train loss:1.5523831319703603\n",
            "=== epoch:230, train acc:0.5766666666666667, test acc:0.4252 ===\n",
            "train loss:1.5105158432369103\n",
            "train loss:1.3890738950574075\n",
            "train loss:1.434676713187432\n",
            "=== epoch:231, train acc:0.57, test acc:0.4262 ===\n",
            "train loss:1.3475059907189786\n",
            "train loss:1.454603712015864\n",
            "train loss:1.3370798691530936\n",
            "=== epoch:232, train acc:0.56, test acc:0.4256 ===\n",
            "train loss:1.4730514153036507\n",
            "train loss:1.35347307642572\n",
            "train loss:1.3465879820270459\n",
            "=== epoch:233, train acc:0.5566666666666666, test acc:0.4286 ===\n",
            "train loss:1.401048251008638\n",
            "train loss:1.5141247587948559\n",
            "train loss:1.4575021751750623\n",
            "=== epoch:234, train acc:0.5666666666666667, test acc:0.4315 ===\n",
            "train loss:1.454200595934899\n",
            "train loss:1.5142538283344142\n",
            "train loss:1.6280786830764844\n",
            "=== epoch:235, train acc:0.58, test acc:0.4333 ===\n",
            "train loss:1.4154196828641432\n",
            "train loss:1.4791206852237675\n",
            "train loss:1.2693786074208542\n",
            "=== epoch:236, train acc:0.5733333333333334, test acc:0.4358 ===\n",
            "train loss:1.3662523064435168\n",
            "train loss:1.5697880658275876\n",
            "train loss:1.424143186398726\n",
            "=== epoch:237, train acc:0.5766666666666667, test acc:0.4354 ===\n",
            "train loss:1.424504883428051\n",
            "train loss:1.379877948223558\n",
            "train loss:1.4802779298613897\n",
            "=== epoch:238, train acc:0.58, test acc:0.4379 ===\n",
            "train loss:1.3056310616097904\n",
            "train loss:1.4291358829145795\n",
            "train loss:1.3169358911826914\n",
            "=== epoch:239, train acc:0.5766666666666667, test acc:0.4358 ===\n",
            "train loss:1.489687842111648\n",
            "train loss:1.3798080526839909\n",
            "train loss:1.4214667780908754\n",
            "=== epoch:240, train acc:0.59, test acc:0.4369 ===\n",
            "train loss:1.4349466811888438\n",
            "train loss:1.3461766117929104\n",
            "train loss:1.4234590466779147\n",
            "=== epoch:241, train acc:0.5833333333333334, test acc:0.4387 ===\n",
            "train loss:1.4019660811745482\n",
            "train loss:1.5440945974289877\n",
            "train loss:1.3428034543901533\n",
            "=== epoch:242, train acc:0.59, test acc:0.4406 ===\n",
            "train loss:1.3837532221925033\n",
            "train loss:1.479877864637628\n",
            "train loss:1.310138733618\n",
            "=== epoch:243, train acc:0.5866666666666667, test acc:0.4405 ===\n",
            "train loss:1.4232666475998874\n",
            "train loss:1.5068736914987804\n",
            "train loss:1.3295983969272334\n",
            "=== epoch:244, train acc:0.5833333333333334, test acc:0.4391 ===\n",
            "train loss:1.3565421769813244\n",
            "train loss:1.3909749216352563\n",
            "train loss:1.2888555961320103\n",
            "=== epoch:245, train acc:0.5833333333333334, test acc:0.4402 ===\n",
            "train loss:1.395859719549212\n",
            "train loss:1.4557060645471374\n",
            "train loss:1.3864910507433887\n",
            "=== epoch:246, train acc:0.5933333333333334, test acc:0.4439 ===\n",
            "train loss:1.4030269130487403\n",
            "train loss:1.3564099917337242\n",
            "train loss:1.315781363272073\n",
            "=== epoch:247, train acc:0.59, test acc:0.4452 ===\n",
            "train loss:1.386893925701883\n",
            "train loss:1.4996570747529836\n",
            "train loss:1.339322527371145\n",
            "=== epoch:248, train acc:0.59, test acc:0.4478 ===\n",
            "train loss:1.4551543064126506\n",
            "train loss:1.4029126585250964\n",
            "train loss:1.21578692511816\n",
            "=== epoch:249, train acc:0.5966666666666667, test acc:0.449 ===\n",
            "train loss:1.4649684488841883\n",
            "train loss:1.3806144636606716\n",
            "train loss:1.3038120447012833\n",
            "=== epoch:250, train acc:0.6033333333333334, test acc:0.4507 ===\n",
            "train loss:1.3788644812421549\n",
            "train loss:1.3294711703109743\n",
            "train loss:1.30191504936694\n",
            "=== epoch:251, train acc:0.6, test acc:0.4492 ===\n",
            "train loss:1.2717318592740678\n",
            "train loss:1.2739946502650468\n",
            "train loss:1.3275879015337213\n",
            "=== epoch:252, train acc:0.5966666666666667, test acc:0.4515 ===\n",
            "train loss:1.3389738198742265\n",
            "train loss:1.2774064178808175\n",
            "train loss:1.311201943289553\n",
            "=== epoch:253, train acc:0.6, test acc:0.4532 ===\n",
            "train loss:1.3329799625934406\n",
            "train loss:1.3171024040724129\n",
            "train loss:1.3680358661093208\n",
            "=== epoch:254, train acc:0.6066666666666667, test acc:0.4529 ===\n",
            "train loss:1.3120156678033386\n",
            "train loss:1.2714651541625197\n",
            "train loss:1.3260991520356358\n",
            "=== epoch:255, train acc:0.6033333333333334, test acc:0.4514 ===\n",
            "train loss:1.3340547427556566\n",
            "train loss:1.4105799141501485\n",
            "train loss:1.43290984307908\n",
            "=== epoch:256, train acc:0.59, test acc:0.4571 ===\n",
            "train loss:1.2760828734535352\n",
            "train loss:1.363430223534082\n",
            "train loss:1.4369978076693997\n",
            "=== epoch:257, train acc:0.6033333333333334, test acc:0.4584 ===\n",
            "train loss:1.293271919393896\n",
            "train loss:1.3189087226290992\n",
            "train loss:1.2658231672170053\n",
            "=== epoch:258, train acc:0.5766666666666667, test acc:0.457 ===\n",
            "train loss:1.3004702825699197\n",
            "train loss:1.3536064875538265\n",
            "train loss:1.2302807061648582\n",
            "=== epoch:259, train acc:0.5933333333333334, test acc:0.4604 ===\n",
            "train loss:1.3720302234232244\n",
            "train loss:1.369345353898659\n",
            "train loss:1.3470080973683725\n",
            "=== epoch:260, train acc:0.5833333333333334, test acc:0.4618 ===\n",
            "train loss:1.3049303576493982\n",
            "train loss:1.2999479914319267\n",
            "train loss:1.270643870917097\n",
            "=== epoch:261, train acc:0.5966666666666667, test acc:0.4648 ===\n",
            "train loss:1.2034584856974795\n",
            "train loss:1.4101524888716777\n",
            "train loss:1.311060327486627\n",
            "=== epoch:262, train acc:0.6066666666666667, test acc:0.4683 ===\n",
            "train loss:1.115886299225457\n",
            "train loss:1.3067683526659726\n",
            "train loss:1.1828256844663374\n",
            "=== epoch:263, train acc:0.6033333333333334, test acc:0.4715 ===\n",
            "train loss:1.2253209086186059\n",
            "train loss:1.2638839690066592\n",
            "train loss:1.3191491044760262\n",
            "=== epoch:264, train acc:0.59, test acc:0.4753 ===\n",
            "train loss:1.2984362136057077\n",
            "train loss:1.2512463599551042\n",
            "train loss:1.2121418851406818\n",
            "=== epoch:265, train acc:0.61, test acc:0.4743 ===\n",
            "train loss:1.3053411386750482\n",
            "train loss:1.2728753652709475\n",
            "train loss:1.1763369073882608\n",
            "=== epoch:266, train acc:0.6, test acc:0.4773 ===\n",
            "train loss:1.2628736098534747\n",
            "train loss:1.2720998884029506\n",
            "train loss:1.2440756005149984\n",
            "=== epoch:267, train acc:0.6133333333333333, test acc:0.4857 ===\n",
            "train loss:1.2894611331908614\n",
            "train loss:1.204591148526067\n",
            "train loss:1.422019772550186\n",
            "=== epoch:268, train acc:0.61, test acc:0.4869 ===\n",
            "train loss:1.2385194985407442\n",
            "train loss:1.227339033453363\n",
            "train loss:1.1200259879370287\n",
            "=== epoch:269, train acc:0.62, test acc:0.4873 ===\n",
            "train loss:1.2816011185804193\n",
            "train loss:1.2193837497040523\n",
            "train loss:1.232746069767027\n",
            "=== epoch:270, train acc:0.6166666666666667, test acc:0.4895 ===\n",
            "train loss:1.294626376510004\n",
            "train loss:1.2584035794924555\n",
            "train loss:1.239545777242175\n",
            "=== epoch:271, train acc:0.61, test acc:0.4891 ===\n",
            "train loss:1.2128344993608657\n",
            "train loss:1.1992165226498361\n",
            "train loss:1.295227081821436\n",
            "=== epoch:272, train acc:0.6166666666666667, test acc:0.4935 ===\n",
            "train loss:1.2754017149621257\n",
            "train loss:1.1977018688376542\n",
            "train loss:1.3063726269831364\n",
            "=== epoch:273, train acc:0.6233333333333333, test acc:0.4925 ===\n",
            "train loss:1.2216584232973298\n",
            "train loss:1.2792743869020735\n",
            "train loss:1.1546741311822428\n",
            "=== epoch:274, train acc:0.63, test acc:0.4989 ===\n",
            "train loss:1.1519005523759482\n",
            "train loss:1.2691969104169738\n",
            "train loss:1.2023738809249855\n",
            "=== epoch:275, train acc:0.6266666666666667, test acc:0.4999 ===\n",
            "train loss:1.2237047025197745\n",
            "train loss:1.0759565780668754\n",
            "train loss:1.173850887320848\n",
            "=== epoch:276, train acc:0.6233333333333333, test acc:0.5005 ===\n",
            "train loss:1.206134174642595\n",
            "train loss:1.1853839261870127\n",
            "train loss:1.0888637814602256\n",
            "=== epoch:277, train acc:0.6233333333333333, test acc:0.4978 ===\n",
            "train loss:1.2629906017647428\n",
            "train loss:1.1560681344090746\n",
            "train loss:1.2369779137967414\n",
            "=== epoch:278, train acc:0.63, test acc:0.497 ===\n",
            "train loss:1.196992238127954\n",
            "train loss:1.1544317067815806\n",
            "train loss:1.0592640800920163\n",
            "=== epoch:279, train acc:0.62, test acc:0.4963 ===\n",
            "train loss:1.1723935019974854\n",
            "train loss:1.0535202888884398\n",
            "train loss:1.2485338537885753\n",
            "=== epoch:280, train acc:0.62, test acc:0.4948 ===\n",
            "train loss:1.2713058843063934\n",
            "train loss:1.2842395539638354\n",
            "train loss:1.288568651210018\n",
            "=== epoch:281, train acc:0.6166666666666667, test acc:0.4975 ===\n",
            "train loss:1.2719272401434083\n",
            "train loss:1.1045501736427625\n",
            "train loss:1.0557211534655482\n",
            "=== epoch:282, train acc:0.6233333333333333, test acc:0.5001 ===\n",
            "train loss:1.205697131252929\n",
            "train loss:1.1968745132718266\n",
            "train loss:1.2088682060753775\n",
            "=== epoch:283, train acc:0.63, test acc:0.4997 ===\n",
            "train loss:1.054490965389649\n",
            "train loss:1.1070782974304303\n",
            "train loss:1.0718272289795059\n",
            "=== epoch:284, train acc:0.6366666666666667, test acc:0.4996 ===\n",
            "train loss:1.1578577493769084\n",
            "train loss:1.166718356292979\n",
            "train loss:1.207901689973112\n",
            "=== epoch:285, train acc:0.6366666666666667, test acc:0.4938 ===\n",
            "train loss:1.1128932991842222\n",
            "train loss:1.0480475648996574\n",
            "train loss:1.1962528051663721\n",
            "=== epoch:286, train acc:0.65, test acc:0.4943 ===\n",
            "train loss:1.0960567104786156\n",
            "train loss:1.1482181117953658\n",
            "train loss:1.100225142137881\n",
            "=== epoch:287, train acc:0.6466666666666666, test acc:0.493 ===\n",
            "train loss:1.1611846339107046\n",
            "train loss:1.1437878183963184\n",
            "train loss:1.1711265617547932\n",
            "=== epoch:288, train acc:0.6533333333333333, test acc:0.4932 ===\n",
            "train loss:1.155370080465298\n",
            "train loss:1.1497669831156223\n",
            "train loss:1.1472738907228024\n",
            "=== epoch:289, train acc:0.67, test acc:0.4956 ===\n",
            "train loss:1.1556056949540807\n",
            "train loss:1.077253443112114\n",
            "train loss:1.034205966181349\n",
            "=== epoch:290, train acc:0.66, test acc:0.4966 ===\n",
            "train loss:1.0921655328493773\n",
            "train loss:1.1490807010966722\n",
            "train loss:1.1117570034510003\n",
            "=== epoch:291, train acc:0.6633333333333333, test acc:0.501 ===\n",
            "train loss:1.1175223680810207\n",
            "train loss:1.1634687636046785\n",
            "train loss:1.1870175461820778\n",
            "=== epoch:292, train acc:0.6733333333333333, test acc:0.5068 ===\n",
            "train loss:1.102368695516265\n",
            "train loss:1.0217925513195574\n",
            "train loss:1.1381901985707252\n",
            "=== epoch:293, train acc:0.6733333333333333, test acc:0.5035 ===\n",
            "train loss:1.0961566615835292\n",
            "train loss:1.1428344317040808\n",
            "train loss:1.1743395766624136\n",
            "=== epoch:294, train acc:0.6766666666666666, test acc:0.5057 ===\n",
            "train loss:1.1243463404474443\n",
            "train loss:1.0847474142825986\n",
            "train loss:1.180288114716821\n",
            "=== epoch:295, train acc:0.6866666666666666, test acc:0.5045 ===\n",
            "train loss:1.1643602346745157\n",
            "train loss:1.1462909443351017\n",
            "train loss:1.047836905348683\n",
            "=== epoch:296, train acc:0.6866666666666666, test acc:0.5044 ===\n",
            "train loss:1.1549237183597563\n",
            "train loss:1.0937472276003182\n",
            "train loss:1.0114856221669664\n",
            "=== epoch:297, train acc:0.67, test acc:0.505 ===\n",
            "train loss:1.0179648586615986\n",
            "train loss:0.9745931217881488\n",
            "train loss:1.138198943259152\n",
            "=== epoch:298, train acc:0.68, test acc:0.5036 ===\n",
            "train loss:1.1610267628625381\n",
            "train loss:0.9950882454207062\n",
            "train loss:1.0301243194603216\n",
            "=== epoch:299, train acc:0.6766666666666666, test acc:0.5045 ===\n",
            "train loss:1.0828003171540914\n",
            "train loss:0.960394225951186\n",
            "train loss:1.1157993145364118\n",
            "=== epoch:300, train acc:0.6833333333333333, test acc:0.5039 ===\n",
            "train loss:1.0541064484127163\n",
            "train loss:1.0824652853183734\n",
            "train loss:1.0731257991605059\n",
            "=== epoch:301, train acc:0.68, test acc:0.5048 ===\n",
            "train loss:1.024892629073556\n",
            "train loss:0.988219109239687\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.5056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC1nSpPvxV3D"
      },
      "source": [
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "from common.util import shuffle_dataset\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist()\n",
        "\n",
        "#훈련 데이터를 뒤섞는다\n",
        "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
        "\n",
        "#20%를 검증 데이터로 분할\n",
        "validation_rate = 0.20\n",
        "validation_num = int(x_train.shape[0] * validation_rate)\n",
        "\n",
        "x_val = x_train[:validation_num]\n",
        "t_val = t_train[:validation_num]\n",
        "x_train = x_train[validation_num:]\n",
        "t_train = t_train[validation_num:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHcwRcyjGiIZ"
      },
      "source": [
        "#7장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sF8jW_6zcGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2202b0-5a88-41d9-feba-85c75cee3c5c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.random.rand(10, 1, 28, 28) #무작위로 데이터 생성\n",
        "\n",
        "print(x.shape)\n",
        "print(x[0].shape)\n",
        "print(x[1].shape)\n",
        "print(x[0, 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 1, 28, 28)\n",
            "(1, 28, 28)\n",
            "(1, 28, 28)\n",
            "[[4.02277095e-01 1.71894235e-01 1.67011324e-01 8.11671748e-01\n",
            "  5.52446798e-01 3.02888654e-01 6.99309973e-01 6.58336663e-01\n",
            "  9.17218072e-01 3.78528659e-01 2.46142065e-02 3.95514732e-01\n",
            "  6.54324816e-01 3.74077055e-01 4.77489196e-01 1.04769115e-01\n",
            "  9.99531559e-01 7.21953253e-01 2.28597163e-01 7.28913167e-01\n",
            "  1.98216770e-01 3.23137838e-01 6.10897570e-01 9.40102576e-01\n",
            "  4.05002447e-01 8.07213338e-02 5.36218097e-01 2.04954174e-01]\n",
            " [3.53510934e-01 9.04930703e-01 7.78707373e-01 8.61208506e-01\n",
            "  1.22390538e-01 2.83747105e-01 3.72980637e-01 2.46601522e-01\n",
            "  3.45622973e-02 1.80965640e-01 6.99125540e-01 5.42852826e-01\n",
            "  5.70282331e-01 7.22546121e-01 9.19235979e-02 1.28206523e-01\n",
            "  8.64571128e-01 6.23834597e-01 7.37661131e-01 6.54682004e-01\n",
            "  6.30866231e-01 9.27620988e-01 5.53391579e-01 5.06020331e-01\n",
            "  3.16054921e-01 6.47419379e-01 3.63282158e-01 5.96491315e-01]\n",
            " [8.40940789e-01 4.86893599e-01 3.04533939e-01 5.36789249e-01\n",
            "  8.94892427e-01 3.16655003e-01 8.97507531e-02 1.70910579e-01\n",
            "  6.76907017e-01 9.58720677e-01 3.13322132e-01 5.63738073e-01\n",
            "  1.40305980e-01 1.11738413e-01 7.18548532e-01 2.65457477e-01\n",
            "  4.81934241e-01 7.49639637e-01 8.67206456e-01 7.54206838e-01\n",
            "  3.88254634e-01 2.97825277e-01 8.27402572e-01 9.98759454e-01\n",
            "  4.86369082e-01 9.54605590e-01 4.18435122e-01 1.25420735e-01]\n",
            " [9.15959903e-01 4.45626888e-01 3.37844544e-01 3.03881709e-02\n",
            "  9.12198085e-01 9.41322263e-01 8.50197577e-02 4.43824264e-01\n",
            "  3.23818079e-01 3.70820855e-02 6.08807666e-01 4.10645381e-01\n",
            "  4.14147275e-01 8.32378516e-01 8.50455666e-01 7.04759369e-01\n",
            "  7.94614066e-01 5.64317439e-01 6.27380191e-01 5.27990911e-01\n",
            "  9.75919323e-03 8.92106547e-01 2.32503573e-01 6.43693913e-01\n",
            "  2.21477446e-01 8.59210701e-01 8.52548557e-01 9.94824559e-01]\n",
            " [1.40370019e-01 6.49166084e-02 8.39610679e-02 8.12963910e-01\n",
            "  6.98515945e-01 5.68890219e-01 2.67149270e-01 5.70765284e-01\n",
            "  5.89496194e-01 3.19315493e-01 6.67109784e-01 9.98346842e-01\n",
            "  8.52020293e-01 9.88504205e-01 3.96843234e-01 7.39746292e-01\n",
            "  1.96307315e-01 5.41329217e-01 5.06934024e-01 5.90940879e-01\n",
            "  3.91778464e-01 5.47574770e-01 7.43612847e-01 3.08432241e-02\n",
            "  6.22887583e-02 8.74301733e-01 9.29952312e-01 5.77468543e-01]\n",
            " [1.63066971e-01 9.85040008e-01 9.55229229e-01 5.26703083e-01\n",
            "  1.89604343e-01 1.07355211e-02 5.72167639e-01 8.34955216e-01\n",
            "  8.36860078e-01 1.79749245e-02 3.68525090e-02 5.16821521e-01\n",
            "  6.63890874e-01 5.01209319e-01 5.77810717e-01 8.15173356e-01\n",
            "  5.37034137e-01 9.86948330e-01 4.32165580e-01 3.36783144e-01\n",
            "  3.40523705e-01 9.67251767e-01 5.35404893e-01 8.72530519e-01\n",
            "  8.11573385e-01 4.07797859e-02 5.27704666e-01 2.04935826e-01]\n",
            " [3.30994859e-01 6.53141088e-01 1.36305309e-01 9.65182606e-01\n",
            "  4.13811948e-01 8.06606959e-01 7.52923093e-01 3.82699927e-01\n",
            "  7.26776727e-01 3.81152940e-01 5.57242790e-01 1.32887558e-02\n",
            "  7.70182361e-01 1.13761436e-01 6.74252068e-01 1.11329376e-01\n",
            "  5.43188227e-01 2.44362703e-01 6.81768109e-01 3.42959598e-01\n",
            "  9.66331404e-01 5.28593733e-01 8.81145611e-01 7.26598507e-01\n",
            "  7.12778633e-02 4.83019370e-02 3.27869262e-01 1.29093431e-01]\n",
            " [8.75194802e-01 4.21749963e-01 4.27350508e-01 2.91145082e-01\n",
            "  2.98159007e-01 2.29740116e-01 8.26550767e-01 6.61122223e-03\n",
            "  6.64519008e-01 8.63756436e-01 9.18437961e-01 5.91702514e-02\n",
            "  8.79091236e-01 3.37620042e-01 2.95979302e-01 9.54582473e-01\n",
            "  2.16328786e-01 3.80954313e-01 5.93064235e-01 4.27728132e-02\n",
            "  6.21037823e-01 4.48169971e-01 9.33434749e-01 2.65570490e-01\n",
            "  7.47383552e-02 5.25223765e-01 2.81233024e-01 1.13671219e-01]\n",
            " [2.66030287e-02 9.12182637e-01 6.74639836e-01 9.01761642e-02\n",
            "  8.21075723e-01 9.16940031e-01 8.14490278e-01 9.95758748e-01\n",
            "  8.12907414e-01 8.59152134e-01 2.33880507e-01 1.25297664e-01\n",
            "  2.24494761e-01 3.59088423e-02 3.65995727e-01 3.02615479e-02\n",
            "  1.10453761e-01 5.47690827e-02 9.51360262e-01 8.49076581e-03\n",
            "  7.78486024e-02 2.78713474e-02 6.28895256e-01 9.40242291e-01\n",
            "  6.56877666e-01 6.84406865e-01 2.97396415e-01 5.04838278e-01]\n",
            " [4.73268994e-01 8.47944547e-01 8.20788519e-01 4.80632599e-01\n",
            "  8.31983876e-01 5.31335157e-01 4.79411791e-01 9.32313148e-01\n",
            "  1.69703698e-01 6.23298906e-01 8.96807479e-01 7.87801778e-01\n",
            "  9.55985658e-01 1.80530174e-01 5.42858189e-01 1.76115594e-01\n",
            "  3.42988593e-01 8.22791954e-01 8.93595655e-01 8.66849488e-02\n",
            "  8.95415233e-01 1.93836795e-02 8.05719930e-01 7.04473250e-01\n",
            "  7.05144885e-01 1.55314201e-01 4.61563811e-01 2.76699599e-01]\n",
            " [2.64494937e-01 4.09804924e-01 7.26486274e-01 8.06647023e-01\n",
            "  6.72638763e-01 1.29119985e-01 9.29729959e-01 2.72444033e-01\n",
            "  2.56658125e-01 1.78740569e-01 3.28893208e-01 9.49598892e-01\n",
            "  5.28462690e-01 7.82714469e-01 9.80082361e-01 5.96935559e-01\n",
            "  4.27837003e-01 2.94370287e-01 7.06834812e-01 1.93461744e-01\n",
            "  4.88843453e-01 3.84477144e-01 1.47357000e-01 1.29263106e-01\n",
            "  5.58254919e-01 5.47264086e-01 4.65156546e-01 5.76050464e-01]\n",
            " [3.00430393e-01 6.66447133e-01 8.27200790e-01 2.09920762e-01\n",
            "  3.95747569e-01 6.05090808e-01 1.80306637e-01 7.35667139e-01\n",
            "  7.63936423e-01 4.98312608e-01 9.84679484e-01 3.54574442e-01\n",
            "  6.31955123e-01 4.10544019e-01 7.53141603e-01 9.53493514e-01\n",
            "  2.98416089e-01 5.93557347e-01 9.64774555e-02 1.43391212e-02\n",
            "  3.86325671e-01 6.97811721e-01 4.13867064e-01 3.29138564e-01\n",
            "  8.76959789e-01 2.76402898e-01 9.03460799e-01 9.82122767e-01]\n",
            " [7.36646583e-01 6.57036964e-01 8.79150321e-01 2.29015998e-01\n",
            "  3.54800539e-01 6.62690004e-01 8.92063365e-03 5.24013408e-01\n",
            "  4.97231603e-02 6.51561844e-01 8.68396442e-01 4.51981675e-01\n",
            "  8.54310408e-01 5.84860740e-01 3.41124158e-01 4.14330447e-01\n",
            "  6.30528997e-01 7.42465833e-02 9.56638937e-01 8.02577652e-01\n",
            "  2.22743021e-01 2.88546578e-01 8.18340124e-02 8.06342885e-01\n",
            "  8.05094553e-01 1.23473846e-01 6.89438136e-01 9.88379312e-01]\n",
            " [1.24378797e-01 5.75420172e-01 2.08117862e-01 4.45100146e-01\n",
            "  6.33871272e-01 7.78152761e-01 9.11392587e-02 6.95209984e-01\n",
            "  2.80402100e-01 2.72464146e-01 9.77486488e-01 9.08543824e-01\n",
            "  7.22833718e-01 9.99892662e-01 1.32185007e-02 6.94507594e-01\n",
            "  8.42215108e-01 6.92369802e-01 2.73028406e-01 5.20348911e-01\n",
            "  5.05612828e-01 8.51322727e-03 7.32582387e-01 7.98333159e-01\n",
            "  6.80218781e-01 2.81550436e-01 3.04522832e-02 8.60413475e-01]\n",
            " [9.57006172e-01 3.35473514e-01 5.03422671e-01 7.31704995e-01\n",
            "  4.83319686e-01 7.98313952e-01 8.89804841e-01 3.80171047e-01\n",
            "  6.19281580e-01 7.07374714e-01 1.70113651e-01 6.98303090e-01\n",
            "  4.84054022e-01 2.08152758e-01 4.36798582e-01 1.82458245e-01\n",
            "  5.90884190e-01 6.01477902e-01 1.71125436e-03 9.42197144e-01\n",
            "  1.14341737e-01 3.28451238e-01 1.43704318e-01 1.04399501e-01\n",
            "  3.10752814e-01 2.81247938e-01 1.17005178e-01 3.74983951e-01]\n",
            " [2.39986645e-02 8.32607467e-01 8.05029544e-01 7.23741469e-01\n",
            "  5.21831822e-01 5.86744902e-01 3.20995662e-01 1.80771638e-01\n",
            "  1.74325484e-01 8.46212850e-01 8.89364569e-02 3.30376865e-01\n",
            "  7.81231462e-01 3.23519147e-01 3.23860876e-01 1.47045672e-01\n",
            "  1.55100266e-01 1.81846958e-01 4.14960139e-01 2.05885001e-01\n",
            "  4.01840610e-01 8.21226695e-01 4.87759150e-01 4.84637671e-01\n",
            "  6.34607698e-01 8.86171491e-01 9.86275171e-01 9.93006566e-01]\n",
            " [3.97617518e-01 4.04920999e-01 4.81899421e-01 9.66159846e-01\n",
            "  8.61560269e-01 5.59597825e-01 7.25651688e-01 8.36676701e-01\n",
            "  9.36586225e-01 3.91205324e-01 2.51988116e-01 7.42232221e-01\n",
            "  9.63005948e-01 9.15840737e-01 6.18992893e-01 3.61120233e-01\n",
            "  6.17080137e-01 3.36659312e-01 7.53112809e-01 3.45165396e-01\n",
            "  3.18141209e-01 9.77792610e-01 5.51706334e-01 9.47206828e-03\n",
            "  7.01268776e-01 1.40717191e-01 1.04330996e-01 4.89255913e-01]\n",
            " [5.18824124e-01 8.98517502e-01 3.86788103e-01 1.19377730e-01\n",
            "  5.00764963e-01 1.50347923e-01 8.94022501e-01 1.47985166e-01\n",
            "  9.22981553e-01 4.66519147e-01 8.73162467e-01 6.88223470e-01\n",
            "  1.08680574e-01 3.44468310e-01 3.31702634e-01 7.27462694e-01\n",
            "  7.92218212e-01 6.05856076e-01 7.84894334e-01 8.31107475e-01\n",
            "  8.95802271e-01 6.66755983e-01 5.22971094e-03 1.26881920e-01\n",
            "  8.45744854e-01 9.07033248e-01 2.07199182e-01 3.61723343e-01]\n",
            " [4.81978375e-01 7.50159241e-01 7.48581265e-02 6.17609907e-01\n",
            "  2.03061196e-01 1.79603065e-01 4.10925570e-01 4.04463846e-01\n",
            "  6.41669298e-01 3.18827740e-01 6.90070152e-01 4.42870680e-01\n",
            "  1.60020217e-01 4.17903044e-01 3.18907370e-01 9.84978809e-01\n",
            "  2.89758011e-01 9.31073820e-02 7.15502673e-01 5.76326888e-01\n",
            "  6.59024262e-01 1.64806783e-01 7.13378780e-01 4.81783030e-02\n",
            "  9.41238860e-01 9.13298077e-01 2.38798638e-01 7.52743203e-01]\n",
            " [8.97585335e-01 1.47954464e-01 1.57379993e-01 5.53964197e-01\n",
            "  4.74872213e-01 5.56797851e-01 9.26805460e-01 5.53930178e-01\n",
            "  5.91808752e-01 7.56038364e-01 6.50062352e-01 5.46129171e-01\n",
            "  1.85329988e-02 3.86327147e-01 8.67244719e-01 1.79922438e-02\n",
            "  7.91217349e-01 2.33681612e-01 6.07616823e-01 5.34904452e-01\n",
            "  1.55735072e-01 8.24589098e-02 3.05581635e-01 8.13314179e-01\n",
            "  9.55694450e-01 8.50589254e-01 6.46880266e-01 8.01781577e-01]\n",
            " [8.12682804e-02 8.52648622e-01 4.92869318e-01 2.20999181e-01\n",
            "  4.12923583e-01 8.19360479e-01 2.33290702e-01 9.68825073e-01\n",
            "  8.72150626e-01 7.64960149e-01 7.65575614e-01 6.86314331e-01\n",
            "  6.53090394e-01 3.24827392e-01 9.93203260e-01 3.53954376e-01\n",
            "  1.84984511e-01 3.42358540e-01 1.25644281e-01 4.56392266e-02\n",
            "  9.04141746e-01 4.69936743e-01 1.52981204e-01 4.29831539e-01\n",
            "  4.60986049e-01 5.18942344e-01 2.63666681e-01 3.69243605e-01]\n",
            " [2.34771181e-01 2.68111781e-01 6.88268400e-02 6.79385857e-01\n",
            "  3.14225642e-01 6.29256550e-01 1.33739923e-01 8.17408310e-01\n",
            "  1.89719504e-01 5.08408081e-01 8.87577559e-01 1.15348753e-02\n",
            "  1.58330128e-01 5.83399335e-01 7.42755993e-01 2.59215617e-01\n",
            "  5.14991303e-01 8.74789369e-01 1.59940012e-01 7.12600672e-01\n",
            "  4.73398228e-01 6.68382375e-01 6.86376753e-01 4.85031325e-01\n",
            "  9.59361717e-01 7.23564823e-01 9.41696787e-01 5.88544888e-01]\n",
            " [2.20575973e-01 5.82921959e-01 8.74686655e-01 9.89850605e-01\n",
            "  3.70393131e-01 5.78349855e-01 4.32280587e-01 1.98062885e-01\n",
            "  6.17852250e-01 7.10673903e-02 9.74089627e-02 5.82666220e-02\n",
            "  9.90394885e-01 2.28793769e-01 3.65821757e-01 5.09447290e-01\n",
            "  4.98694326e-01 7.37204475e-01 5.65803050e-01 5.66846837e-01\n",
            "  7.60364911e-01 2.03622494e-01 7.46129637e-01 7.91834416e-01\n",
            "  3.40898450e-01 4.18107040e-01 1.79428020e-01 8.77014260e-01]\n",
            " [8.64268580e-01 6.99230648e-01 5.05634103e-01 5.63789190e-01\n",
            "  6.20889049e-01 5.97954377e-01 3.52440159e-01 2.09419356e-01\n",
            "  3.16980450e-01 6.57480419e-01 3.00474016e-01 3.22089596e-01\n",
            "  8.57457993e-02 8.31096782e-01 6.77131044e-01 6.43801657e-02\n",
            "  9.72548087e-01 5.99270410e-01 5.22185358e-01 8.78782097e-01\n",
            "  6.92944327e-01 5.11317803e-01 5.45189378e-01 2.82843921e-01\n",
            "  8.40261341e-01 9.69485072e-03 9.37088895e-01 4.17740087e-01]\n",
            " [8.08902888e-01 7.70577540e-01 8.82066144e-01 5.06977119e-01\n",
            "  8.56582910e-01 3.78953744e-02 3.20154001e-01 9.71913413e-01\n",
            "  8.68626211e-01 3.43110908e-01 4.49032076e-01 3.27686354e-01\n",
            "  1.11168731e-01 8.41934532e-02 2.97660500e-01 5.04202895e-01\n",
            "  9.73859914e-01 2.84488758e-01 2.21118241e-01 8.60889116e-01\n",
            "  4.86057927e-02 2.74818635e-01 6.61240489e-02 9.01025357e-01\n",
            "  4.44662834e-01 3.65621186e-01 5.58587330e-01 2.41817516e-01]\n",
            " [7.05268413e-01 1.93551740e-01 8.52555894e-02 4.07578534e-05\n",
            "  1.49833756e-01 1.54264506e-01 9.07040829e-01 1.59539412e-01\n",
            "  4.45851466e-01 7.31607286e-01 6.16028705e-01 4.25687013e-01\n",
            "  4.90388835e-01 4.55391155e-01 1.09574656e-01 5.91822442e-01\n",
            "  5.69751186e-01 7.17822856e-01 3.72074930e-01 7.97198143e-01\n",
            "  9.07173355e-01 6.60286392e-01 3.55626072e-01 6.30164151e-01\n",
            "  8.85751093e-01 8.62601209e-01 7.43246934e-01 8.35370361e-01]\n",
            " [2.88588862e-01 6.83680771e-01 1.23248072e-01 7.69694840e-01\n",
            "  4.73138349e-01 6.05762239e-01 6.71414692e-01 9.21653166e-01\n",
            "  2.79489860e-02 1.92205702e-02 1.45125641e-01 2.40836055e-01\n",
            "  5.58127160e-01 5.29488283e-01 6.86209472e-01 3.93216803e-01\n",
            "  4.18366053e-01 5.49322984e-01 5.79783871e-01 4.08756605e-01\n",
            "  8.74492161e-01 9.40736383e-01 4.32644405e-01 5.00256885e-01\n",
            "  1.61466793e-01 5.72531208e-01 1.92032336e-02 7.73460320e-01]\n",
            " [3.19209663e-01 4.19372271e-01 4.57767462e-01 9.01908615e-01\n",
            "  5.22594356e-01 7.45462748e-01 5.83572210e-01 9.63766515e-01\n",
            "  8.65030947e-01 6.45422726e-02 1.42947215e-01 6.86106687e-01\n",
            "  8.25218166e-02 1.85057005e-01 2.13357481e-01 9.34229371e-01\n",
            "  6.84935798e-01 2.37317420e-01 3.48146879e-01 9.87758804e-01\n",
            "  8.48914831e-01 3.71128008e-01 4.84887819e-01 7.18699565e-01\n",
            "  8.16105153e-01 8.00005231e-01 5.13104547e-01 6.50019981e-02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XXJ9_Z6GzCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a2970d-5384-4045-8a51-b2d1978caca3"
      },
      "source": [
        "import numpy as np\n",
        "import sys,os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master\")\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-master')\n",
        "\n",
        "from common.util import im2col\n",
        "\n",
        "x1 = np.random.rand(1, 3, 7 , 7) #(데이터수, 채널 수, 높이, 너비)\n",
        "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
        "print(col1.shape)\n",
        "\n",
        "x2 = np.random.rand(10, 3, 7 , 7)\n",
        "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
        "print(col2.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9, 75)\n",
            "(90, 75)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsxWjOaDMoEU"
      },
      "source": [
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    col : 2차원 배열\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oje40SLtISJf"
      },
      "source": [
        "class Convolution:\n",
        "  def __init__(self, W, b, stride=1, pad=0):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "  def forward(self, x):\n",
        "    FN, C, FH, FW = self.W.shape    #필터\n",
        "    N, C, H, W = x.shape            #입력 데이터\n",
        "    \n",
        "    out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
        "    out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "    col = im2col(x, FH, self.stride, self.pad)      #입력데이터를 4차원에서 2차원으로\n",
        "    col_W = self.W.reshape(FN, -1).T    #필터 전개 / reshape(-1, FN)해도 ok\n",
        "    \n",
        "    out = np.dot(col, col_W) + self.b   #행렬곱 편향더하기 / 주의 편향은 삼차원 \n",
        "\n",
        "    out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)        #2차원인 결과를 4차원으로 바꾸기\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZKfUXS_KDJd"
      },
      "source": [
        "import numpy as np\n",
        "from common.util import im2col\n",
        "\n",
        "class Pooling:\n",
        "  def __init__(self, pool_h, pool_w, stride=1,pad=0):\n",
        "    self.pool_h = pool_h\n",
        "    self.pool_w = pool_w\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "  def forward(self, x):\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = int(1 + (H + 2*self.pad - self.pool_h) / self.stride)\n",
        "    out_w = int(1 + (W + 2*self.pad - self.pool_w) / self.stride)\n",
        "\n",
        "    #전개 (1)\n",
        "    col = im2col(x, self.pool_h, self.pool_w, slef.stride, self.pad)\n",
        "    col = col.reshape(-1, self.pool_h * self.pool_w)\n",
        "\n",
        "    #최댓값 (2)     각 행의 최댓값을 저장하겠다 세로 한 줄로\n",
        "    out = np.max(col, axis=1)\n",
        "\n",
        "    #성형 (3)\n",
        "    out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOmwTWgLkOfN"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGE1iXNnOfym"
      },
      "source": [
        "class SimpleConvNet:\n",
        "  def __init__(self, input_dim=(1,28,28),\n",
        "               conv_param = {'filter_num':30, 'filter_size':5, 'pad':0,'stride':1}, \n",
        "               hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "    \n",
        "    filter_num = conv_param['filter_num']\n",
        "    filter_size = conv_param['filter_size']\n",
        "    filter_pad = conv_param['pad']\n",
        "    filter_stride = conv_param['stride']\n",
        "    \n",
        "    input_size = input_dim[1]\n",
        "    conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "    pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))        #affine층의 가중치 형상을 맞춰주려고 계산\n",
        "                                        # 풀링이 2*2라서 2로 나눠준다.\n",
        "    #가중치 매개변수 초기화\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "    self.params['b1'] = np.zeros(filter_num)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(pool_out_size, hidden_size)\n",
        "    self.params['b2'] = np.zeros(hidden_size)\n",
        "    self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "    self.layers = OrderedDict()\n",
        "    self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
        "    self.layers['Relu1'] = Relu()\n",
        "    self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "    self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "    self.layers['Relu2'] = Relu()\n",
        "    self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "    self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "  \n",
        "  def predict(self, x):\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "      return x\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return self.last_layer.forward(y, t)\n",
        "\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    #순전파\n",
        "    self.loss(x, t)\n",
        "\n",
        "    #역전파\n",
        "    dout = 1\n",
        "    dout = self.last_layer.backward(dout)\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "    \n",
        "    #결과 저장\n",
        "    grads = {}\n",
        "    grads['W1'] = self.layers['Conv1'].dw\n",
        "    grads['b1'] = self.layers['Conv1'].db\n",
        "    grads['W2'] = self.layers['Affine1'].dw\n",
        "    grads['b2'] = self.layers['Affine1'].db\n",
        "    grads['W3'] = self.layers['Affine2'].dw\n",
        "    grads['b3'] = self.layers['Affine2'].db\n",
        "\n",
        "    return grads\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBRKjtltUg9c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}